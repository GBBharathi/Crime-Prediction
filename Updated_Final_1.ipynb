{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Output Old model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Knife', 1: 'Handgun'}\n",
      "()\n",
      "[24502]\n",
      "()\n",
      "[24502]\n",
      "()\n",
      "[24502]\n",
      "Current Time: 2024-03-20 20:49:24.671503\n",
      "Hours: 20\n",
      "Minutes: 49\n",
      "Seconds: 24\n",
      "Day: 20\n",
      "Month: 3\n",
      "Year: 2024\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_lat_long_by_ip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 219\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMonth: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 219\u001b[0m result \u001b[38;5;241m=\u001b[39m get_lat_long_by_ip()\n\u001b[0;32m    220\u001b[0m latitude, longitude \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude = \u001b[39m\u001b[38;5;124m'\u001b[39m,latitude)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_lat_long_by_ip' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import cv2\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "import time\n",
    "\n",
    "with open(\"C:/Users/Gnana Bharathi S/Downloads/dataset/dataset.yaml\", mode='r') as f:\n",
    "    data_yaml = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "labels = data_yaml['names']\n",
    "print(labels)\n",
    "\n",
    "# Load YOLO model for gun detection\n",
    "#yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/Dec 14/gun_knife_jan_30.onnx')\n",
    "yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/gun_knife_jan_30.onnx')\n",
    "#yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/Dec 14/knife.onnx')\n",
    "#yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/Gun_Knife20/weights/best.onnx')\n",
    "yolo.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "yolo.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "# Load emotion detection model\n",
    "model = DeepFace.build_model(\"Emotion\")\n",
    "\n",
    "# Define emotion labels\n",
    "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "\n",
    "# Load face cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Open video capture\n",
    "#video_path = 'C:/Users/Gnana Bharathi S/Downloads/demo2.mp4'  # Update with your video file path\n",
    "#video_path='C:/Users/Gnana Bharathi S/Downloads/feb8vid3.mp4'\n",
    "video_path='C:/Users/Gnana Bharathi S/Downloads/feb8vid1.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "#cap = cv2.VideoCapture(0)\n",
    "start_time = None\n",
    "\n",
    "\n",
    "#yolo_knife = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/Dec 14/knife.onnx')\n",
    "yolo_knife = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/knife.onnx')\n",
    "#yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/feb3.onnx')\n",
    "yolo_knife.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "yolo_knife.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video stream\")\n",
    "        break\n",
    "\n",
    "    image = frame.copy()\n",
    "\n",
    "    row, col, d = image.shape  # height, width, depth\n",
    "    max_rc = max(row, col)\n",
    "    input_image = np.zeros((max_rc, max_rc, 3), dtype=np.uint8)\n",
    "    input_image[0:row, 0:col] = image\n",
    "\n",
    "    INPUT_WH_YOLO = 640\n",
    "    blob = cv2.dnn.blobFromImage(input_image, 1/255, (INPUT_WH_YOLO, INPUT_WH_YOLO), swapRB=True, crop=False)\n",
    "    yolo.setInput(blob)\n",
    "    preds = yolo.forward()\n",
    "\n",
    "    detections = preds[0]\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classes = []\n",
    "\n",
    "    image_w, image_h = input_image.shape[:2]\n",
    "    x_fac = image_w / INPUT_WH_YOLO\n",
    "    y_fac = image_h / INPUT_WH_YOLO\n",
    "\n",
    "    for i in range(len(detections)):\n",
    "        row = detections[i]\n",
    "        confidence = row[4]\n",
    "        if confidence > 0:\n",
    "            class_score = row[5:].max()\n",
    "            class_id = row[5:].argmax()\n",
    "            if class_score > 0.25:\n",
    "                cx, cy, w, h = row[0:4]\n",
    "                left = int((cx - 0.5 * w) * x_fac)\n",
    "                top = int((cy - 0.5 * h) * y_fac)\n",
    "                width = int(w * x_fac)\n",
    "                height = int(h * y_fac)\n",
    "                box = np.array([left, top, width, height])\n",
    "                confidences.append(confidence)\n",
    "                boxes.append(box)\n",
    "                classes.append(class_id)\n",
    "\n",
    "    boxes_np = np.array(boxes).tolist()\n",
    "    confidences_np = np.array(confidences).tolist()\n",
    "\n",
    "    result = cv2.dnn.NMSBoxes(boxes_np, confidences_np, 0.3, 0.2)\n",
    "    print(result)\n",
    "    if len(result) > 0:\n",
    "        flattened_result = result.flatten()\n",
    "        for ind in flattened_result:\n",
    "            x, y, w, h = boxes_np[ind]\n",
    "            bb_conf = int(confidences_np[ind] * 100)\n",
    "            classes_id = classes[ind]\n",
    "            class_name = labels[classes_id]\n",
    "\n",
    "            # Draw a rectangle around the gun and label it as 'gun'=='knife'\n",
    "            if('gun' in class_name):\n",
    "                cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 255), 2)\n",
    "                cv2.putText(image, 'Gun', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 255, 255), 1)\n",
    "                #cv2.putText(image, f'Gun {bb_conf}% confidence', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 255, 255), 1)\n",
    "    else:\n",
    "        yolo_knife.setInput(blob)\n",
    "        preds = yolo_knife.forward()\n",
    "\n",
    "        detections = preds[0]\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        classes = []\n",
    "\n",
    "        image_w, image_h = input_image.shape[:2]\n",
    "        x_fac = image_w / INPUT_WH_YOLO\n",
    "        y_fac = image_h / INPUT_WH_YOLO\n",
    "\n",
    "        for i in range(len(detections)):\n",
    "            row = detections[i]\n",
    "            confidence = row[4]\n",
    "            if confidence > 0:\n",
    "                class_score = row[5:].max()\n",
    "                class_id = row[5:].argmax()\n",
    "                if class_score > 0.25:\n",
    "                    cx, cy, w, h = row[0:4]\n",
    "                    left = int((cx - 0.5 * w) * x_fac)\n",
    "                    top = int((cy - 0.5 * h) * y_fac)\n",
    "                    width = int(w * x_fac)\n",
    "                    height = int(h * y_fac)\n",
    "                    box = np.array([left, top, width, height])\n",
    "                    confidences.append(confidence)\n",
    "                    boxes.append(box)\n",
    "                    classes.append(class_id)\n",
    "\n",
    "        boxes_np = np.array(boxes).tolist()\n",
    "        confidences_np = np.array(confidences).tolist()\n",
    "\n",
    "        result = cv2.dnn.NMSBoxes(boxes_np, confidences_np, 0.3, 0.2)\n",
    "        print(result)\n",
    "        if len(result) > 0:\n",
    "            flattened_result = result.flatten()\n",
    "            for ind in flattened_result:\n",
    "                x, y, w, h = boxes_np[ind]\n",
    "                bb_conf = int(confidences_np[ind] * 100)\n",
    "                classes_id = classes[ind]\n",
    "                class_name = labels[classes_id]\n",
    "\n",
    "                # Draw a rectangle around the gun and label it as 'gun'\n",
    "                #print('bb_conf = ',bb_conf)\n",
    "                cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "                cv2.putText(image, 'Knife', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 255), 1)\n",
    "                #cv2.putText(image, f'Knife {bb_conf}% confidence', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 255), 1)\n",
    "\n",
    "    # Detect emotion of the person in the frame\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_roi = gray_frame[y:y + h, x:x + w]\n",
    "        resized_face = cv2.resize(face_roi, (48, 48), interpolation=cv2.INTER_AREA)\n",
    "        normalized_face = resized_face / 255.0\n",
    "        reshaped_face = normalized_face.reshape(1, 48, 48, 1)\n",
    "        preds = model.predict(reshaped_face)[0]\n",
    "        emotion_idx = preds.argmax()\n",
    "        emotion = emotion_labels[emotion_idx]\n",
    "        print(emotion)\n",
    "        confidence_percentage = preds[emotion_idx] * 100\n",
    "        # If gun is detected and emotion is sad or angry, stop running the camera after 3 seconds\n",
    "        if start_time is None:\n",
    "            start_time = time.time()  # Store the current time\n",
    "        else:\n",
    "            current_time = time.time()  # Get the current time\n",
    "            elapsed_time = current_time - start_time  # Calculate the elapsed time\n",
    "            if elapsed_time > 60:  # Check if 30 seconds have passed\n",
    "                print(emotion)\n",
    "                cap.release()  # Release the camera\n",
    "                cv2.destroyAllWindows()  # Close the windows\n",
    "                break  # Break out of the while loop\n",
    "\n",
    "        # Draw a rectangle around the emotion and label it with predicted emotion\n",
    "        color = (0,0,255) if emotion == 'angry' else (0, 0, 255)\n",
    "        text = emotion\n",
    "        #text = f'{emotion}: {confidence_percentage:.2f}%'\n",
    "        if(confidence_percentage<90):\n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), color, 2)\n",
    "            cv2.putText(image, text, (x, y+100), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "    cv2.imshow(\"Video\", image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "current_time = datetime.now()\n",
    "hours = current_time.hour\n",
    "minutes = current_time.minute\n",
    "seconds = current_time.second\n",
    "day=month=current_time.day\n",
    "month=current_time.month\n",
    "year=current_time.year\n",
    "\n",
    "# Print the extracted values\n",
    "print(f\"Current Time: {current_time}\")\n",
    "print(f\"Hours: {hours}\")\n",
    "print(f\"Minutes: {minutes}\")\n",
    "print(f\"Seconds: {seconds}\")\n",
    "print(f\"Day: {day}\")\n",
    "print(f\"Month: {month}\")\n",
    "print(f\"Year: {year}\")\n",
    "\n",
    "result = get_lat_long_by_ip()\n",
    "latitude, longitude = result\n",
    "print('latitude = ',latitude)\n",
    "print('longitude = ',longitude)\n",
    "x, y = convert_lat_lon_to_xy(latitude, longitude)\n",
    "print(f\"X Coordinate: {x}, Y Coordinate: {y}\")\n",
    "\n",
    "# Replace these values with your own data\n",
    "#new_data = np.array([[0,0,latitude, longitude, x, y, year, month, day, hours, minutes,seconds]])\n",
    "new_data = np.array([[1,2,41.830482, -87.621752, 1182280, 1881621, 2023, 12, 15, 14, 30,0]])\n",
    "#new_data=np.array([[0,1,41.747610,-87.549179,1198234.0,1851595.0,2020,7,1,10,16,0]])\n",
    "# Standardize the new data using the same scaler\n",
    "#new_data_standardized = scaler.transform(new_data)\n",
    "\n",
    "# Make predictions for the new data\n",
    "new_data_prediction = loaded_model.predict(new_data)\n",
    "\n",
    "print(f'Predicted Alert for the new data: {new_data_prediction[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude: 41.0878, Longitude: -87.2785\n",
      "1140059.4051471204 1854810.5772076007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gnana Bharathi S\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Safe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_lat_long_by_ip():\n",
    "    try:\n",
    "        response = requests.get('https://ipinfo.io/json')\n",
    "        data = response.json()\n",
    "\n",
    "        if 'loc' in data:\n",
    "            latitude, longitude = map(float, data['loc'].split(','))\n",
    "            latitude+=28\n",
    "            longitude+=7\n",
    "            longitude*=-1\n",
    "            return latitude, longitude\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "result = get_lat_long_by_ip()\n",
    "\n",
    "if result:\n",
    "    latitude, longitude = result\n",
    "    print(f\"Latitude: {latitude}, Longitude: {longitude}\")\n",
    "else:\n",
    "    print(\"Could not retrieve location information.\")\n",
    "import math\n",
    "\n",
    "def convert_lat_lon_to_xy(latitude, longitude):\n",
    "    # Radius of the Earth in meters\n",
    "    R = 6371000.0\n",
    "    \n",
    "    # Convert latitude and longitude to radians\n",
    "    lat_rad = math.radians(latitude)\n",
    "    lon_rad = math.radians(longitude)\n",
    "    \n",
    "    #y Calculate x and y coordinates using Mercator projection\n",
    "    x = R * lon_rad\n",
    "    y = R * math.log(math.tan(math.pi / 4 + lat_rad / 2))\n",
    "    \n",
    "    return x, y\n",
    "x,y=convert_lat_lon_to_xy(41.0878,-87.2785)\n",
    "x*=-1\n",
    "x-=8564867\n",
    "y-=3164867\n",
    "print(x,y)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "import joblib\n",
    "loaded_model = joblib.load(\"C:/Users/Gnana Bharathi S/Downloads/Dec 14/RF1_81.pkl\")\n",
    "new_data=np.array([[0,3,41.840482,-87.621752,1178180.0,1881621.0,2020,10,20,12,23,0]])\n",
    "# Standardize the new data using the same scaler\n",
    "#new_data_standardized = scaler.transform(new_data)\n",
    "\n",
    "# Make predictions for the new data\n",
    "new_data_prediction = loaded_model.predict(new_data)\n",
    "new_data_prediction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Output new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Gnana Bharathi S\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "['gun', 'knife']\n",
      "()\n",
      "[24990]\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "[24128]\n",
      "1/1 [==============================] - 0s 432ms/step\n",
      "sad\n",
      "()\n",
      "()\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "neutral\n",
      "()\n",
      "[22697]\n",
      "[24148]\n",
      "[24168]\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "fear\n",
      "[23179]\n",
      "()\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "fear\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "sad\n",
      "[23132]\n",
      "[23134]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "sad\n",
      "[24148]\n",
      "()\n",
      "()\n",
      "[19406 14842]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "sad\n",
      "[19529 19781]\n",
      "[19569  2442]\n",
      "()\n",
      "()\n",
      "[19933]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "fear\n",
      "[19893]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "fear\n",
      "[21492]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "fear\n",
      "[24165]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "neutral\n",
      "Current Time: 2024-04-16 21:42:07.461367\n",
      "Hours: 21\n",
      "Minutes: 42\n",
      "Seconds: 7\n",
      "Day: 16\n",
      "Month: 4\n",
      "Year: 2024\n",
      "latitude =  41.0878\n",
      "longitude =  -87.2785\n",
      "X Coordinate: -9704926.40514712, Y Coordinate: 5019677.577207601\n",
      "0\n",
      "4\n",
      "Predicted Alert for the new data: Danger\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import cv2\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "import time\n",
    "\n",
    "with open(\"C:/Users/Gnana Bharathi S/Downloads/Gun_Knife/weights/names.yaml\", mode='r') as f:\n",
    "    data_yaml = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "labels = data_yaml['names']\n",
    "print(labels)\n",
    "# Load YOLO model for gun detection\n",
    "yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/Weapon Model/best.onnx')\n",
    "#yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/Gun_Knifea03/weights/best.onnx')\n",
    "#yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/feb3.onnx')\n",
    "yolo.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "yolo.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "# Load emotion detection model\n",
    "model = DeepFace.build_model(\"Emotion\")\n",
    "\n",
    "# Define emotion labels\n",
    "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "\n",
    "# Load face cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Open video capture\n",
    "#video_path = 'C:/Users/Gnana Bharathi S/Downloads/demo2.mp4'  # Update with your video file path\n",
    "#video_path='C:/Users/Gnana Bharathi S/Downloads/feb8vid3.mp4'\n",
    "video_path='C:/Users/Gnana Bharathi S/Downloads/feb8vid.mp4'\n",
    "#cap = cv2.VideoCapture(video_path)\n",
    "cap = cv2.VideoCapture(0)\n",
    "start_time = None\n",
    "\n",
    "\n",
    "'''yolo_knife = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/Dec 14/knife.onnx')\n",
    "#yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/feb3.onnx')\n",
    "yolo_knife.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "yolo_knife.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "'''\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video stream\")\n",
    "        break\n",
    "\n",
    "    image = frame.copy()\n",
    "\n",
    "    row, col, d = image.shape  # height, width, depth\n",
    "    max_rc = max(row, col)\n",
    "    input_image = np.zeros((max_rc, max_rc, 3), dtype=np.uint8)\n",
    "    input_image[0:row, 0:col] = image\n",
    "\n",
    "    INPUT_WH_YOLO = 640\n",
    "    blob = cv2.dnn.blobFromImage(input_image, 1/255, (INPUT_WH_YOLO, INPUT_WH_YOLO), swapRB=True, crop=False)\n",
    "    yolo.setInput(blob)\n",
    "    preds = yolo.forward()\n",
    "\n",
    "    detections = preds[0]\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classes = []\n",
    "\n",
    "    image_w, image_h = input_image.shape[:2]\n",
    "    x_fac = image_w / INPUT_WH_YOLO\n",
    "    y_fac = image_h / INPUT_WH_YOLO\n",
    "\n",
    "    for i in range(len(detections)):\n",
    "        row = detections[i]\n",
    "        confidence = row[4]\n",
    "        if confidence > 0:\n",
    "            class_score = row[5:].max()\n",
    "            class_id = row[5:].argmax()\n",
    "            if class_score > 0.25:\n",
    "                cx, cy, w, h = row[0:4]\n",
    "                left = int((cx - 0.5 * w) * x_fac)\n",
    "                top = int((cy - 0.5 * h) * y_fac)\n",
    "                width = int(w * x_fac)\n",
    "                height = int(h * y_fac)\n",
    "                box = np.array([left, top, width, height])\n",
    "                confidences.append(confidence)\n",
    "                boxes.append(box)\n",
    "                classes.append(class_id)\n",
    "\n",
    "    boxes_np = np.array(boxes).tolist()\n",
    "    confidences_np = np.array(confidences).tolist()\n",
    "\n",
    "    result = cv2.dnn.NMSBoxes(boxes_np, confidences_np, 0.3, 0.2)\n",
    "    print(result)\n",
    "    if len(result) > 0:\n",
    "        flattened_result = result.flatten()\n",
    "        for ind in flattened_result:\n",
    "            x, y, w, h = boxes_np[ind]\n",
    "            bb_conf = int(confidences_np[ind] * 100)\n",
    "            classes_id = classes[ind]\n",
    "            class_name = labels[classes_id]\n",
    "            if('gun' in class_name):\n",
    "                wep=0\n",
    "            elif('knife' in class_name):\n",
    "                wep=1\n",
    "            else:\n",
    "                wep=2\n",
    "            #text=class_name+' = '+str(bb_conf)+'%'\n",
    "            text=class_name\n",
    "            # Draw a rectangle around the gun and label it as 'gun'=='knife'\n",
    "            if('gun' in class_name):\n",
    "                cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "                cv2.putText(image, text, (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 255), 1)\n",
    "            elif('knife' in class_name):\n",
    "                cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 255), 2)\n",
    "                cv2.putText(image, text, (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 255, 255), 1)\n",
    "                \n",
    "                #cv2.putText(image, f'Gun {bb_conf}% confidence', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 255, 255), 1)\n",
    "    '''else:\n",
    "        yolo_knife.setInput(blob)\n",
    "        preds = yolo_knife.forward()\n",
    "\n",
    "        detections = preds[0]\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        classes = []\n",
    "\n",
    "        image_w, image_h = input_image.shape[:2]\n",
    "        x_fac = image_w / INPUT_WH_YOLO\n",
    "        y_fac = image_h / INPUT_WH_YOLO\n",
    "\n",
    "        for i in range(len(detections)):\n",
    "            row = detections[i]\n",
    "            confidence = row[4]\n",
    "            if confidence > 0:\n",
    "                class_score = row[5:].max()\n",
    "                class_id = row[5:].argmax()\n",
    "                if class_score > 0.25:\n",
    "                    cx, cy, w, h = row[0:4]\n",
    "                    left = int((cx - 0.5 * w) * x_fac)\n",
    "                    top = int((cy - 0.5 * h) * y_fac)\n",
    "                    width = int(w * x_fac)\n",
    "                    height = int(h * y_fac)\n",
    "                    box = np.array([left, top, width, height])\n",
    "                    confidences.append(confidence)\n",
    "                    boxes.append(box)\n",
    "                    classes.append(class_id)\n",
    "\n",
    "        boxes_np = np.array(boxes).tolist()\n",
    "        confidences_np = np.array(confidences).tolist()\n",
    "\n",
    "        result = cv2.dnn.NMSBoxes(boxes_np, confidences_np, 0.3, 0.2)\n",
    "        print(result)\n",
    "        if len(result) > 0:\n",
    "            flattened_result = result.flatten()\n",
    "            for ind in flattened_result:\n",
    "                x, y, w, h = boxes_np[ind]\n",
    "                bb_conf = int(confidences_np[ind] * 100)\n",
    "                classes_id = classes[ind]\n",
    "                class_name = labels[classes_id]\n",
    "\n",
    "                # Draw a rectangle around the gun and label it as 'gun'\n",
    "                #print('bb_conf = ',bb_conf)\n",
    "                cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "                cv2.putText(image, 'Knife', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 255), 1)\n",
    "                #cv2.putText(image, f'Knife {bb_conf}% confidence', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 255), 1)'''      \n",
    "\n",
    "    # Detect emotion of the person in the frame\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_roi = gray_frame[y:y + h, x:x + w]\n",
    "        resized_face = cv2.resize(face_roi, (48, 48), interpolation=cv2.INTER_AREA)\n",
    "        normalized_face = resized_face / 255.0\n",
    "        reshaped_face = normalized_face.reshape(1, 48, 48, 1)\n",
    "        preds = model.predict(reshaped_face)[0]\n",
    "        emotion_idx = preds.argmax()\n",
    "        emotion = emotion_labels[emotion_idx]\n",
    "        print(emotion)\n",
    "        confidence_percentage = preds[emotion_idx] * 100\n",
    "        # If gun is detected and emotion is sad or angry, stop running the camera after 3 seconds\n",
    "        if start_time is None:\n",
    "            start_time = time.time()  # Store the current time\n",
    "        else:\n",
    "            current_time = time.time()  # Get the current time\n",
    "            elapsed_time = current_time - start_time  # Calculate the elapsed time\n",
    "            if elapsed_time > 1000:  # Check if 30 seconds have passed\n",
    "                print(emotion)\n",
    "                cap.release()  # Release the camera\n",
    "                cv2.destroyAllWindows()  # Close the windows\n",
    "                break  # Break out of the while loop\n",
    "\n",
    "        # Draw a rectangle around the emotion and label it with predicted emotion\n",
    "        color = (0,0,255) if emotion == 'angry' else (0, 0, 255)\n",
    "        text = emotion\n",
    "        #text = f'{emotion}: {confidence_percentage:.2f}%'\n",
    "        if(confidence_percentage<90):\n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), color, 2)\n",
    "            cv2.putText(image, text, (x, y+100), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "    cv2.imshow(\"Video\", image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "from datetime import datetime\n",
    "current_time = datetime.now()\n",
    "hours = current_time.hour\n",
    "minutes = current_time.minute\n",
    "seconds = current_time.second\n",
    "day=month=current_time.day\n",
    "month=current_time.month\n",
    "year=current_time.year\n",
    "\n",
    "# Print the extracted values\n",
    "print(f\"Current Time: {current_time}\")\n",
    "print(f\"Hours: {hours}\")\n",
    "print(f\"Minutes: {minutes}\")\n",
    "print(f\"Seconds: {seconds}\")\n",
    "print(f\"Day: {day}\")\n",
    "print(f\"Month: {month}\")\n",
    "print(f\"Year: {year}\")\n",
    "\n",
    "result = get_lat_long_by_ip()\n",
    "latitude, longitude = result\n",
    "print('latitude = ',latitude)\n",
    "print('longitude = ',longitude)\n",
    "x, y = convert_lat_lon_to_xy(latitude, longitude)\n",
    "print(f\"X Coordinate: {x}, Y Coordinate: {y}\")\n",
    "em=0\n",
    "if(emotion=='angry'):\n",
    "    em=0\n",
    "elif(emotion=='disgust'):\n",
    "    em=1\n",
    "elif(emotion=='fear'):\n",
    "    em=2\n",
    "elif(emotion=='happy'):\n",
    "    em=3\n",
    "elif(emotion=='neutral'):\n",
    "    em=4\n",
    "elif(emotion=='sad'):\n",
    "    em=5\n",
    "elif(emotion=='surprise'):\n",
    "    em=6\n",
    "else:\n",
    "    em=4\n",
    "print(wep)\n",
    "print(em)\n",
    "\n",
    "# Replace these values with your own data\n",
    "new_data = np.array([[wep,em,latitude, longitude, x, y, year, month, day, hours, minutes,seconds]])\n",
    "#new_data = np.array([[1,2,41.830482, -87.621752, 1182280, 1881621, 2023, 12, 15, 14, 30,0]])\n",
    "#new_data=np.array([[0,1,41.747610,-87.549179,1198234.0,1851595.0,2020,7,1,10,16,0]])\n",
    "# Standardize the new data using the same scaler\n",
    "#new_data_standardized = scaler.transform(new_data)\n",
    "\n",
    "# Make predictions for the new data\n",
    "new_data_prediction = loaded_model.predict(new_data)\n",
    "\n",
    "print(f'Predicted Alert for the new data: {new_data_prediction[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gun, Knife, Police"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gun', 'knife']\n",
      "['badge', 'baton', 'duty belt', 'handcuffs', 'lights', 'nameplate', 'officer', 'patch', 'police hat', 'radio', 'sunglasses', 'text', 'vehicle', 'watch']\n",
      "police =  officer:87%\n",
      "police =  officer:81%\n",
      "()\n",
      "police =  officer:87%\n",
      "police =  officer:80%\n",
      "()\n",
      "police =  officer:87%\n",
      "police =  officer:80%\n",
      "()\n",
      "police =  officer:87%\n",
      "police =  officer:80%\n",
      "()\n",
      "Safe since police is present\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import cv2\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "import time\n",
    "\n",
    "with open(\"C:/Users/Gnana Bharathi S/Downloads/Gun_Knife/weights/names.yaml\", mode='r') as f:\n",
    "    data_yaml = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "labels = data_yaml['names']\n",
    "print(labels)\n",
    "\n",
    "\n",
    "\n",
    "with open(\"C:/Users/Gnana Bharathi S/Downloads/Feb23_Police/data_1.yaml\", mode='r') as f:\n",
    "    pol_yaml = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "labels1 = pol_yaml['names']\n",
    "print(labels1)\n",
    "\n",
    "# Load YOLO model for gun detection\n",
    "yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/Weapon Model/best.onnx')\n",
    "#yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/Dec 14/knife.onnx')\n",
    "#yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/feb3.onnx')\n",
    "yolo.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "yolo.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "# Load emotion detection model\n",
    "model = DeepFace.build_model(\"Emotion\")\n",
    "\n",
    "# Define emotion labels\n",
    "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "\n",
    "# Load face cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Open video capture\n",
    "#video_path = 'C:/Users/Gnana Bharathi S/Downloads/demo2.mp4'  # Update with your video file path\n",
    "#video_path='C:/Users/Gnana Bharathi S/Downloads/feb8vid3.mp4'\n",
    "video_path='C:/Users/Gnana Bharathi S/Downloads/pol4.mp4'\n",
    "#video_path=\"C:/Users/Gnana Bharathi S/Videos/20240313_110450.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "#cap = cv2.VideoCapture(0)\n",
    "start_time = None\n",
    "\n",
    "\n",
    "'''yolo_knife = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/Dec 14/knife.onnx')\n",
    "#yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/feb3.onnx')\n",
    "yolo_knife.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "yolo_knife.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)'''\n",
    "frame_skip = 5  # Process every 5th frame\n",
    "frame_count = 0\n",
    "\n",
    "#Police\n",
    "yolo_p = cv2.dnn.readNetFromONNX(\"C:/Users/Gnana Bharathi S/Downloads/Feb23_Police/Feb23_Police.onnx\")\n",
    "yolo_p.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "yolo_p.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "gun=2\n",
    "knife=0\n",
    "em=0\n",
    "emotion=''\n",
    "wep=2\n",
    "pol=0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video stream\")\n",
    "        break\n",
    "    #frame_count += 1\n",
    "    #if frame_count % frame_skip != 0:\n",
    "     #   continue\n",
    "\n",
    "    image = frame.copy()\n",
    "    resize_width = 480\n",
    "    resize_height = 480\n",
    "    input_image = cv2.resize(image, (resize_width, resize_height))\n",
    "    row, col, d = image.shape  # height, width, depth\n",
    "    max_rc = max(row, col)\n",
    "    input_image = np.zeros((max_rc, max_rc, 3), dtype=np.uint8)\n",
    "    \n",
    "    input_image[0:row, 0:col] = image\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    INPUT_WH_YOLO = 640\n",
    "    blob = cv2.dnn.blobFromImage(input_image, 1 / 255, (INPUT_WH_YOLO, INPUT_WH_YOLO), swapRB=True, crop=False)\n",
    "    yolo_p.setInput(blob)\n",
    "    preds = yolo_p.forward()\n",
    "\n",
    "    detections = preds[0]\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classes = []\n",
    "\n",
    "    image_w, image_h = input_image.shape[:2]\n",
    "    x_fac = image_w / INPUT_WH_YOLO\n",
    "    y_fac = image_h / INPUT_WH_YOLO\n",
    "\n",
    "    for i in range(len(detections)):\n",
    "        row = detections[i]\n",
    "        confidence = row[4]\n",
    "        if confidence > 0:\n",
    "            class_score = row[5:].max()\n",
    "            class_id = row[5:].argmax()\n",
    "            if class_score > 0.25:\n",
    "                cx, cy, w, h = row[0:4]\n",
    "                left = int((cx - 0.5 * w) * x_fac)\n",
    "                top = int((cy - 0.5 * h) * y_fac)\n",
    "                width = int(w * x_fac)\n",
    "                height = int(h * y_fac)\n",
    "                box = np.array([left, top, width, height])\n",
    "                confidences.append(confidence)\n",
    "                boxes.append(box)\n",
    "                classes.append(class_id)\n",
    "\n",
    "    boxes_np = np.array(boxes).tolist()\n",
    "    confidences_np = np.array(confidences).tolist()\n",
    "\n",
    "    result = cv2.dnn.NMSBoxes(boxes_np, confidences_np, 0.3, 0.2)\n",
    "\n",
    "    if len(result) > 0:\n",
    "        flattened_result = result.flatten()\n",
    "        for ind in flattened_result:\n",
    "            x, y, w, h = boxes_np[ind]\n",
    "            bb_conf = int(confidences_np[ind] * 100)\n",
    "            classes_id = classes[ind]\n",
    "            class_name = labels1[classes_id]\n",
    "            text = f'{class_name}:{bb_conf}%'\n",
    "            if(('officer' in class_name.lower() or 'text' in class_name.lower()) and bb_conf>70):\n",
    "                cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.putText(image, text, (x, y+20), cv2.FONT_HERSHEY_PLAIN, 0.7, (255, 255, 255), 1)\n",
    "                print('police = ',text)\n",
    "                pol=1\n",
    "            #else:\n",
    "             #   pol=0\n",
    "            #if('officer' in class_name.lower() or 'text' in class_name.lower()):\n",
    "             #   pol=1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    INPUT_WH_YOLO = 640\n",
    "    blob = cv2.dnn.blobFromImage(input_image, 1/255, (INPUT_WH_YOLO, INPUT_WH_YOLO), swapRB=True, crop=False)\n",
    "    yolo.setInput(blob)\n",
    "    preds = yolo.forward()\n",
    "\n",
    "    detections = preds[0]\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classes = []\n",
    "\n",
    "    image_w, image_h = input_image.shape[:2]\n",
    "    x_fac = image_w / INPUT_WH_YOLO\n",
    "    y_fac = image_h / INPUT_WH_YOLO\n",
    "\n",
    "    for i in range(len(detections)):\n",
    "        row = detections[i]\n",
    "        confidence = row[4]\n",
    "        if confidence > 0:\n",
    "            class_score = row[5:].max()\n",
    "            class_id = row[5:].argmax()\n",
    "            if class_score > 0.25:\n",
    "                cx, cy, w, h = row[0:4]\n",
    "                left = int((cx - 0.5 * w) * x_fac)\n",
    "                top = int((cy - 0.5 * h) * y_fac)\n",
    "                width = int(w * x_fac)\n",
    "                height = int(h * y_fac)\n",
    "                box = np.array([left, top, width, height])\n",
    "                confidences.append(confidence)\n",
    "                boxes.append(box)\n",
    "                classes.append(class_id)\n",
    "\n",
    "    boxes_np = np.array(boxes).tolist()\n",
    "    confidences_np = np.array(confidences).tolist()\n",
    "\n",
    "    result = cv2.dnn.NMSBoxes(boxes_np, confidences_np, 0.3, 0.2)\n",
    "    print(result)\n",
    "    if len(result) > 0:\n",
    "        flattened_result = result.flatten()\n",
    "        gun=1\n",
    "        wep=0\n",
    "        for ind in flattened_result:\n",
    "            x, y, w, h = boxes_np[ind]\n",
    "            bb_conf = int(confidences_np[ind] * 100)\n",
    "            classes_id = classes[ind]\n",
    "            class_name = labels[classes_id]\n",
    "            print(class_name)\n",
    "            if('gun' in class_name or 'handgun' in class_name):\n",
    "            # Draw a rectangle around the gun and label it as 'gun'\n",
    "                print(bb_conf)\n",
    "                if(bb_conf>0):\n",
    "                    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "                    cv2.putText(image, 'Gun', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 255), 1)\n",
    "                    #cv2.putText(image, f'Gun {bb_conf}% confidence', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 255), 1)\n",
    "            elif('knife' in class_name):\n",
    "                if(bb_conf>0):\n",
    "                    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "                    cv2.putText(image, 'Knife', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 255), 1)\n",
    "                    #cv2.putText(image, f'Gun {bb_conf}% confidence', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 255), 1)\n",
    "                \n",
    "    '''else:\n",
    "        yolo_knife.setInput(blob)\n",
    "        preds = yolo_knife.forward()\n",
    "\n",
    "        detections = preds[0]\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        classes = []\n",
    "\n",
    "        image_w, image_h = input_image.shape[:2]\n",
    "        x_fac = image_w / INPUT_WH_YOLO\n",
    "        y_fac = image_h / INPUT_WH_YOLO\n",
    "\n",
    "        for i in range(len(detections)):\n",
    "            row = detections[i]\n",
    "            confidence = row[4]\n",
    "            if confidence > 0:\n",
    "                class_score = row[5:].max()\n",
    "                class_id = row[5:].argmax()\n",
    "                if class_score > 0.25:\n",
    "                    cx, cy, w, h = row[0:4]\n",
    "                    left = int((cx - 0.5 * w) * x_fac)\n",
    "                    top = int((cy - 0.5 * h) * y_fac)\n",
    "                    width = int(w * x_fac)\n",
    "                    height = int(h * y_fac)\n",
    "                    box = np.array([left, top, width, height])\n",
    "                    confidences.append(confidence)\n",
    "                    boxes.append(box)\n",
    "                    classes.append(class_id)\n",
    "\n",
    "        boxes_np = np.array(boxes).tolist()\n",
    "        confidences_np = np.array(confidences).tolist()\n",
    "\n",
    "        result = cv2.dnn.NMSBoxes(boxes_np, confidences_np, 0.3, 0.2)\n",
    "        print(result)\n",
    "        if len(result) > 0:\n",
    "            flattened_result = result.flatten()\n",
    "            knife=1\n",
    "            wep=1\n",
    "            for ind in flattened_result:\n",
    "                x, y, w, h = boxes_np[ind]\n",
    "                bb_conf = int(confidences_np[ind] * 100)\n",
    "                classes_id = classes[ind]\n",
    "                class_name = labels[classes_id]\n",
    "\n",
    "                # Draw a rectangle around the gun and label it as 'gun'\n",
    "                print('bb_conf = ',bb_conf)\n",
    "                cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "                cv2.putText(image, 'Knife', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 255), 1)\n",
    "                #cv2.putText(image, f'Knife {bb_conf}% confidence', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 255), 1)'''\n",
    "\n",
    "    # Detect emotion of the person in the frame\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_roi = gray_frame[y:y + h, x:x + w]\n",
    "        resized_face = cv2.resize(face_roi, (48, 48), interpolation=cv2.INTER_AREA)\n",
    "        normalized_face = resized_face / 255.0\n",
    "        reshaped_face = normalized_face.reshape(1, 48, 48, 1)\n",
    "        preds = model.predict(reshaped_face)[0]\n",
    "        emotion_idx = preds.argmax()\n",
    "        emotion = emotion_labels[emotion_idx]\n",
    "        print(emotion)\n",
    "        confidence_percentage = preds[emotion_idx] * 100\n",
    "        # If gun is detected and emotion is sad or angry, stop running the camera after 3 seconds\n",
    "        if start_time is None:\n",
    "            start_time = time.time()  # Store the current time\n",
    "        else:\n",
    "            current_time = time.time()  # Get the current time\n",
    "            elapsed_time = current_time - start_time  # Calculate the elapsed time\n",
    "            if elapsed_time > 1000:  # Check if 5 seconds have passed\n",
    "                print(emotion)\n",
    "                cap.release()  # Release the camera\n",
    "                cv2.destroyAllWindows()  # Close the windows\n",
    "                break  # Break out of the while loop\n",
    "\n",
    "        # Draw a rectangle around the emotion and label it with predicted emotion\n",
    "        color = (0,0,255) if emotion == 'angry' else (0, 0, 255)\n",
    "        text = emotion\n",
    "        #text = f'{emotion}: {confidence_percentage:.2f}%'\n",
    "        if(confidence_percentage<80):\n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), color, 2)\n",
    "            cv2.putText(image, text, (x, y+100), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "    cv2.imshow(\"Video\", image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "if(pol==0):\n",
    "    from datetime import datetime\n",
    "    current_time = datetime.now()\n",
    "    hours = current_time.hour\n",
    "    minutes = current_time.minute\n",
    "    seconds = current_time.second\n",
    "    day=month=current_time.day\n",
    "    month=current_time.month\n",
    "    year=current_time.year\n",
    "\n",
    "    # Print the extracted values\n",
    "    #print(f\"Current Time: {current_time}\")\n",
    "    #print(f\"Hours: {hours}\")\n",
    "    #print(f\"Minutes: {minutes}\")\n",
    "    #print(f\"Seconds: {seconds}\")\n",
    "    #print(f\"Day: {day}\")\n",
    "    #print(f\"Month: {month}\")\n",
    "    #print(f\"Year: {year}\")\n",
    "\n",
    "    result = get_lat_long_by_ip()\n",
    "    latitude, longitude = result\n",
    "    #latitude = 41\n",
    "    #longitude=-87\n",
    "    #print('latitude = ',latitude)\n",
    "    #print('longitude = ',longitude)\n",
    "    x, y = convert_lat_lon_to_xy(latitude, longitude)\n",
    "    #x=1182280\n",
    "    #y=1881621\n",
    "    #print(f\"X Coordinate: {x}, Y Coordinate: {y}\")\n",
    "\n",
    "    if(emotion=='angry'):\n",
    "        em=0\n",
    "    elif(emotion=='disgust'):\n",
    "        em=1\n",
    "    elif(emotion=='fear'):\n",
    "        em=2\n",
    "    elif(emotion=='happy'):\n",
    "        em=3\n",
    "    elif(emotion=='neutral'):\n",
    "        em=4\n",
    "    elif(emotion=='sad'):\n",
    "        em=5\n",
    "    elif(emotion=='surprise'):\n",
    "        em=6\n",
    "    else:\n",
    "        em=4\n",
    "    # Replace these values with your own data\n",
    "    new_data = np.array([[wep,em,latitude, longitude, x, y, year, month, day, hours, minutes,seconds]])\n",
    "    #new_data = np.array([[1,2,41.830482, -87.621752, 1182280, 1881621, 2023, 12, 15, 14, 30,0]])\n",
    "    #new_data=np.array([[0,1,41.747610,-87.549179,1198234.0,1851595.0,2020,7,1,10,16,0]])\n",
    "    # Standardize the new data using the same scaler\n",
    "    #new_data_standardized = scaler.transform(new_data)\n",
    "\n",
    "    # Make predictions for the new data\n",
    "    new_data_prediction = loaded_model.predict(new_data)\n",
    "\n",
    "    print(f'Predicted Alert for the new data: {new_data_prediction[0]}')\n",
    "else:\n",
    "    print('Safe since police is present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Gnana Bharathi S\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import cv2\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph main_graph (\n",
      "  %images[FLOAT, 1x3x640x640]\n",
      ") initializers (\n",
      "  %model.0.conv.weight[FLOAT, 32x3x6x6]\n",
      "  %model.0.conv.bias[FLOAT, 32]\n",
      "  %model.1.conv.weight[FLOAT, 64x32x3x3]\n",
      "  %model.1.conv.bias[FLOAT, 64]\n",
      "  %model.2.cv1.conv.weight[FLOAT, 32x64x1x1]\n",
      "  %model.2.cv1.conv.bias[FLOAT, 32]\n",
      "  %model.2.cv2.conv.weight[FLOAT, 32x64x1x1]\n",
      "  %model.2.cv2.conv.bias[FLOAT, 32]\n",
      "  %model.2.cv3.conv.weight[FLOAT, 64x64x1x1]\n",
      "  %model.2.cv3.conv.bias[FLOAT, 64]\n",
      "  %model.2.m.0.cv1.conv.weight[FLOAT, 32x32x1x1]\n",
      "  %model.2.m.0.cv1.conv.bias[FLOAT, 32]\n",
      "  %model.2.m.0.cv2.conv.weight[FLOAT, 32x32x3x3]\n",
      "  %model.2.m.0.cv2.conv.bias[FLOAT, 32]\n",
      "  %model.3.conv.weight[FLOAT, 128x64x3x3]\n",
      "  %model.3.conv.bias[FLOAT, 128]\n",
      "  %model.4.cv1.conv.weight[FLOAT, 64x128x1x1]\n",
      "  %model.4.cv1.conv.bias[FLOAT, 64]\n",
      "  %model.4.cv2.conv.weight[FLOAT, 64x128x1x1]\n",
      "  %model.4.cv2.conv.bias[FLOAT, 64]\n",
      "  %model.4.cv3.conv.weight[FLOAT, 128x128x1x1]\n",
      "  %model.4.cv3.conv.bias[FLOAT, 128]\n",
      "  %model.4.m.0.cv1.conv.weight[FLOAT, 64x64x1x1]\n",
      "  %model.4.m.0.cv1.conv.bias[FLOAT, 64]\n",
      "  %model.4.m.0.cv2.conv.weight[FLOAT, 64x64x3x3]\n",
      "  %model.4.m.0.cv2.conv.bias[FLOAT, 64]\n",
      "  %model.4.m.1.cv1.conv.weight[FLOAT, 64x64x1x1]\n",
      "  %model.4.m.1.cv1.conv.bias[FLOAT, 64]\n",
      "  %model.4.m.1.cv2.conv.weight[FLOAT, 64x64x3x3]\n",
      "  %model.4.m.1.cv2.conv.bias[FLOAT, 64]\n",
      "  %model.5.conv.weight[FLOAT, 256x128x3x3]\n",
      "  %model.5.conv.bias[FLOAT, 256]\n",
      "  %model.6.cv1.conv.weight[FLOAT, 128x256x1x1]\n",
      "  %model.6.cv1.conv.bias[FLOAT, 128]\n",
      "  %model.6.cv2.conv.weight[FLOAT, 128x256x1x1]\n",
      "  %model.6.cv2.conv.bias[FLOAT, 128]\n",
      "  %model.6.cv3.conv.weight[FLOAT, 256x256x1x1]\n",
      "  %model.6.cv3.conv.bias[FLOAT, 256]\n",
      "  %model.6.m.0.cv1.conv.weight[FLOAT, 128x128x1x1]\n",
      "  %model.6.m.0.cv1.conv.bias[FLOAT, 128]\n",
      "  %model.6.m.0.cv2.conv.weight[FLOAT, 128x128x3x3]\n",
      "  %model.6.m.0.cv2.conv.bias[FLOAT, 128]\n",
      "  %model.6.m.1.cv1.conv.weight[FLOAT, 128x128x1x1]\n",
      "  %model.6.m.1.cv1.conv.bias[FLOAT, 128]\n",
      "  %model.6.m.1.cv2.conv.weight[FLOAT, 128x128x3x3]\n",
      "  %model.6.m.1.cv2.conv.bias[FLOAT, 128]\n",
      "  %model.6.m.2.cv1.conv.weight[FLOAT, 128x128x1x1]\n",
      "  %model.6.m.2.cv1.conv.bias[FLOAT, 128]\n",
      "  %model.6.m.2.cv2.conv.weight[FLOAT, 128x128x3x3]\n",
      "  %model.6.m.2.cv2.conv.bias[FLOAT, 128]\n",
      "  %model.7.conv.weight[FLOAT, 512x256x3x3]\n",
      "  %model.7.conv.bias[FLOAT, 512]\n",
      "  %model.8.cv1.conv.weight[FLOAT, 256x512x1x1]\n",
      "  %model.8.cv1.conv.bias[FLOAT, 256]\n",
      "  %model.8.cv2.conv.weight[FLOAT, 256x512x1x1]\n",
      "  %model.8.cv2.conv.bias[FLOAT, 256]\n",
      "  %model.8.cv3.conv.weight[FLOAT, 512x512x1x1]\n",
      "  %model.8.cv3.conv.bias[FLOAT, 512]\n",
      "  %model.8.m.0.cv1.conv.weight[FLOAT, 256x256x1x1]\n",
      "  %model.8.m.0.cv1.conv.bias[FLOAT, 256]\n",
      "  %model.8.m.0.cv2.conv.weight[FLOAT, 256x256x3x3]\n",
      "  %model.8.m.0.cv2.conv.bias[FLOAT, 256]\n",
      "  %model.9.cv1.conv.weight[FLOAT, 256x512x1x1]\n",
      "  %model.9.cv1.conv.bias[FLOAT, 256]\n",
      "  %model.9.cv2.conv.weight[FLOAT, 512x1024x1x1]\n",
      "  %model.9.cv2.conv.bias[FLOAT, 512]\n",
      "  %model.10.conv.weight[FLOAT, 256x512x1x1]\n",
      "  %model.10.conv.bias[FLOAT, 256]\n",
      "  %model.13.cv1.conv.weight[FLOAT, 128x512x1x1]\n",
      "  %model.13.cv1.conv.bias[FLOAT, 128]\n",
      "  %model.13.cv2.conv.weight[FLOAT, 128x512x1x1]\n",
      "  %model.13.cv2.conv.bias[FLOAT, 128]\n",
      "  %model.13.cv3.conv.weight[FLOAT, 256x256x1x1]\n",
      "  %model.13.cv3.conv.bias[FLOAT, 256]\n",
      "  %model.13.m.0.cv1.conv.weight[FLOAT, 128x128x1x1]\n",
      "  %model.13.m.0.cv1.conv.bias[FLOAT, 128]\n",
      "  %model.13.m.0.cv2.conv.weight[FLOAT, 128x128x3x3]\n",
      "  %model.13.m.0.cv2.conv.bias[FLOAT, 128]\n",
      "  %model.14.conv.weight[FLOAT, 128x256x1x1]\n",
      "  %model.14.conv.bias[FLOAT, 128]\n",
      "  %model.17.cv1.conv.weight[FLOAT, 64x256x1x1]\n",
      "  %model.17.cv1.conv.bias[FLOAT, 64]\n",
      "  %model.17.cv2.conv.weight[FLOAT, 64x256x1x1]\n",
      "  %model.17.cv2.conv.bias[FLOAT, 64]\n",
      "  %model.17.cv3.conv.weight[FLOAT, 128x128x1x1]\n",
      "  %model.17.cv3.conv.bias[FLOAT, 128]\n",
      "  %model.17.m.0.cv1.conv.weight[FLOAT, 64x64x1x1]\n",
      "  %model.17.m.0.cv1.conv.bias[FLOAT, 64]\n",
      "  %model.17.m.0.cv2.conv.weight[FLOAT, 64x64x3x3]\n",
      "  %model.17.m.0.cv2.conv.bias[FLOAT, 64]\n",
      "  %model.18.conv.weight[FLOAT, 128x128x3x3]\n",
      "  %model.18.conv.bias[FLOAT, 128]\n",
      "  %model.20.cv1.conv.weight[FLOAT, 128x256x1x1]\n",
      "  %model.20.cv1.conv.bias[FLOAT, 128]\n",
      "  %model.20.cv2.conv.weight[FLOAT, 128x256x1x1]\n",
      "  %model.20.cv2.conv.bias[FLOAT, 128]\n",
      "  %model.20.cv3.conv.weight[FLOAT, 256x256x1x1]\n",
      "  %model.20.cv3.conv.bias[FLOAT, 256]\n",
      "  %model.20.m.0.cv1.conv.weight[FLOAT, 128x128x1x1]\n",
      "  %model.20.m.0.cv1.conv.bias[FLOAT, 128]\n",
      "  %model.20.m.0.cv2.conv.weight[FLOAT, 128x128x3x3]\n",
      "  %model.20.m.0.cv2.conv.bias[FLOAT, 128]\n",
      "  %model.21.conv.weight[FLOAT, 256x256x3x3]\n",
      "  %model.21.conv.bias[FLOAT, 256]\n",
      "  %model.23.cv1.conv.weight[FLOAT, 256x512x1x1]\n",
      "  %model.23.cv1.conv.bias[FLOAT, 256]\n",
      "  %model.23.cv2.conv.weight[FLOAT, 256x512x1x1]\n",
      "  %model.23.cv2.conv.bias[FLOAT, 256]\n",
      "  %model.23.cv3.conv.weight[FLOAT, 512x512x1x1]\n",
      "  %model.23.cv3.conv.bias[FLOAT, 512]\n",
      "  %model.23.m.0.cv1.conv.weight[FLOAT, 256x256x1x1]\n",
      "  %model.23.m.0.cv1.conv.bias[FLOAT, 256]\n",
      "  %model.23.m.0.cv2.conv.weight[FLOAT, 256x256x3x3]\n",
      "  %model.23.m.0.cv2.conv.bias[FLOAT, 256]\n",
      "  %model.24.m.0.weight[FLOAT, 57x128x1x1]\n",
      "  %model.24.m.0.bias[FLOAT, 57]\n",
      "  %model.24.m.1.weight[FLOAT, 57x256x1x1]\n",
      "  %model.24.m.1.bias[FLOAT, 57]\n",
      "  %model.24.m.2.weight[FLOAT, 57x512x1x1]\n",
      "  %model.24.m.2.bias[FLOAT, 57]\n",
      "  %/model.11/Constant_output_0[FLOAT, 4]\n",
      "  %/model.11/Constant_1_output_0[FLOAT, 0]\n",
      "  %/model.24/Constant_output_0[INT64, 5]\n",
      "  %/model.24/Constant_1_output_0[FLOAT, scalar]\n",
      "  %/model.24/Constant_2_output_0[FLOAT, 1x3x80x80x2]\n",
      "  %/model.24/Constant_3_output_0[FLOAT, scalar]\n",
      "  %/model.24/Constant_6_output_0[FLOAT, 1x3x80x80x2]\n",
      "  %/model.24/Constant_7_output_0[INT64, 3]\n",
      "  %/model.24/Constant_8_output_0[INT64, 5]\n",
      "  %/model.24/Constant_10_output_0[FLOAT, 1x3x40x40x2]\n",
      "  %/model.24/Constant_11_output_0[FLOAT, scalar]\n",
      "  %/model.24/Constant_14_output_0[FLOAT, 1x3x40x40x2]\n",
      "  %/model.24/Constant_15_output_0[INT64, 3]\n",
      "  %/model.24/Constant_16_output_0[INT64, 5]\n",
      "  %/model.24/Constant_18_output_0[FLOAT, 1x3x20x20x2]\n",
      "  %/model.24/Constant_19_output_0[FLOAT, scalar]\n",
      "  %/model.24/Constant_22_output_0[FLOAT, 1x3x20x20x2]\n",
      "  %/model.24/Constant_23_output_0[INT64, 3]\n",
      ") {\n",
      "  %/model.0/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [6, 6], pads = [2, 2, 2, 2], strides = [2, 2]](%images, %model.0.conv.weight, %model.0.conv.bias)\n",
      "  %/model.0/act/Sigmoid_output_0 = Sigmoid(%/model.0/conv/Conv_output_0)\n",
      "  %/model.0/act/Mul_output_0 = Mul(%/model.0/conv/Conv_output_0, %/model.0/act/Sigmoid_output_0)\n",
      "  %/model.1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%/model.0/act/Mul_output_0, %model.1.conv.weight, %model.1.conv.bias)\n",
      "  %/model.1/act/Sigmoid_output_0 = Sigmoid(%/model.1/conv/Conv_output_0)\n",
      "  %/model.1/act/Mul_output_0 = Mul(%/model.1/conv/Conv_output_0, %/model.1/act/Sigmoid_output_0)\n",
      "  %/model.2/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.1/act/Mul_output_0, %model.2.cv1.conv.weight, %model.2.cv1.conv.bias)\n",
      "  %/model.2/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.2/cv1/conv/Conv_output_0)\n",
      "  %/model.2/cv1/act/Mul_output_0 = Mul(%/model.2/cv1/conv/Conv_output_0, %/model.2/cv1/act/Sigmoid_output_0)\n",
      "  %/model.2/m/m.0/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.2/cv1/act/Mul_output_0, %model.2.m.0.cv1.conv.weight, %model.2.m.0.cv1.conv.bias)\n",
      "  %/model.2/m/m.0/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.2/m/m.0/cv1/conv/Conv_output_0)\n",
      "  %/model.2/m/m.0/cv1/act/Mul_output_0 = Mul(%/model.2/m/m.0/cv1/conv/Conv_output_0, %/model.2/m/m.0/cv1/act/Sigmoid_output_0)\n",
      "  %/model.2/m/m.0/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/model.2/m/m.0/cv1/act/Mul_output_0, %model.2.m.0.cv2.conv.weight, %model.2.m.0.cv2.conv.bias)\n",
      "  %/model.2/m/m.0/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.2/m/m.0/cv2/conv/Conv_output_0)\n",
      "  %/model.2/m/m.0/cv2/act/Mul_output_0 = Mul(%/model.2/m/m.0/cv2/conv/Conv_output_0, %/model.2/m/m.0/cv2/act/Sigmoid_output_0)\n",
      "  %/model.2/m/m.0/Add_output_0 = Add(%/model.2/cv1/act/Mul_output_0, %/model.2/m/m.0/cv2/act/Mul_output_0)\n",
      "  %/model.2/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.1/act/Mul_output_0, %model.2.cv2.conv.weight, %model.2.cv2.conv.bias)\n",
      "  %/model.2/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.2/cv2/conv/Conv_output_0)\n",
      "  %/model.2/cv2/act/Mul_output_0 = Mul(%/model.2/cv2/conv/Conv_output_0, %/model.2/cv2/act/Sigmoid_output_0)\n",
      "  %/model.2/Concat_output_0 = Concat[axis = 1](%/model.2/m/m.0/Add_output_0, %/model.2/cv2/act/Mul_output_0)\n",
      "  %/model.2/cv3/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.2/Concat_output_0, %model.2.cv3.conv.weight, %model.2.cv3.conv.bias)\n",
      "  %/model.2/cv3/act/Sigmoid_output_0 = Sigmoid(%/model.2/cv3/conv/Conv_output_0)\n",
      "  %/model.2/cv3/act/Mul_output_0 = Mul(%/model.2/cv3/conv/Conv_output_0, %/model.2/cv3/act/Sigmoid_output_0)\n",
      "  %/model.3/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%/model.2/cv3/act/Mul_output_0, %model.3.conv.weight, %model.3.conv.bias)\n",
      "  %/model.3/act/Sigmoid_output_0 = Sigmoid(%/model.3/conv/Conv_output_0)\n",
      "  %/model.3/act/Mul_output_0 = Mul(%/model.3/conv/Conv_output_0, %/model.3/act/Sigmoid_output_0)\n",
      "  %/model.4/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.3/act/Mul_output_0, %model.4.cv1.conv.weight, %model.4.cv1.conv.bias)\n",
      "  %/model.4/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.4/cv1/conv/Conv_output_0)\n",
      "  %/model.4/cv1/act/Mul_output_0 = Mul(%/model.4/cv1/conv/Conv_output_0, %/model.4/cv1/act/Sigmoid_output_0)\n",
      "  %/model.4/m/m.0/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.4/cv1/act/Mul_output_0, %model.4.m.0.cv1.conv.weight, %model.4.m.0.cv1.conv.bias)\n",
      "  %/model.4/m/m.0/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.4/m/m.0/cv1/conv/Conv_output_0)\n",
      "  %/model.4/m/m.0/cv1/act/Mul_output_0 = Mul(%/model.4/m/m.0/cv1/conv/Conv_output_0, %/model.4/m/m.0/cv1/act/Sigmoid_output_0)\n",
      "  %/model.4/m/m.0/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/model.4/m/m.0/cv1/act/Mul_output_0, %model.4.m.0.cv2.conv.weight, %model.4.m.0.cv2.conv.bias)\n",
      "  %/model.4/m/m.0/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.4/m/m.0/cv2/conv/Conv_output_0)\n",
      "  %/model.4/m/m.0/cv2/act/Mul_output_0 = Mul(%/model.4/m/m.0/cv2/conv/Conv_output_0, %/model.4/m/m.0/cv2/act/Sigmoid_output_0)\n",
      "  %/model.4/m/m.0/Add_output_0 = Add(%/model.4/cv1/act/Mul_output_0, %/model.4/m/m.0/cv2/act/Mul_output_0)\n",
      "  %/model.4/m/m.1/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.4/m/m.0/Add_output_0, %model.4.m.1.cv1.conv.weight, %model.4.m.1.cv1.conv.bias)\n",
      "  %/model.4/m/m.1/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.4/m/m.1/cv1/conv/Conv_output_0)\n",
      "  %/model.4/m/m.1/cv1/act/Mul_output_0 = Mul(%/model.4/m/m.1/cv1/conv/Conv_output_0, %/model.4/m/m.1/cv1/act/Sigmoid_output_0)\n",
      "  %/model.4/m/m.1/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/model.4/m/m.1/cv1/act/Mul_output_0, %model.4.m.1.cv2.conv.weight, %model.4.m.1.cv2.conv.bias)\n",
      "  %/model.4/m/m.1/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.4/m/m.1/cv2/conv/Conv_output_0)\n",
      "  %/model.4/m/m.1/cv2/act/Mul_output_0 = Mul(%/model.4/m/m.1/cv2/conv/Conv_output_0, %/model.4/m/m.1/cv2/act/Sigmoid_output_0)\n",
      "  %/model.4/m/m.1/Add_output_0 = Add(%/model.4/m/m.0/Add_output_0, %/model.4/m/m.1/cv2/act/Mul_output_0)\n",
      "  %/model.4/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.3/act/Mul_output_0, %model.4.cv2.conv.weight, %model.4.cv2.conv.bias)\n",
      "  %/model.4/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.4/cv2/conv/Conv_output_0)\n",
      "  %/model.4/cv2/act/Mul_output_0 = Mul(%/model.4/cv2/conv/Conv_output_0, %/model.4/cv2/act/Sigmoid_output_0)\n",
      "  %/model.4/Concat_output_0 = Concat[axis = 1](%/model.4/m/m.1/Add_output_0, %/model.4/cv2/act/Mul_output_0)\n",
      "  %/model.4/cv3/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.4/Concat_output_0, %model.4.cv3.conv.weight, %model.4.cv3.conv.bias)\n",
      "  %/model.4/cv3/act/Sigmoid_output_0 = Sigmoid(%/model.4/cv3/conv/Conv_output_0)\n",
      "  %/model.4/cv3/act/Mul_output_0 = Mul(%/model.4/cv3/conv/Conv_output_0, %/model.4/cv3/act/Sigmoid_output_0)\n",
      "  %/model.5/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%/model.4/cv3/act/Mul_output_0, %model.5.conv.weight, %model.5.conv.bias)\n",
      "  %/model.5/act/Sigmoid_output_0 = Sigmoid(%/model.5/conv/Conv_output_0)\n",
      "  %/model.5/act/Mul_output_0 = Mul(%/model.5/conv/Conv_output_0, %/model.5/act/Sigmoid_output_0)\n",
      "  %/model.6/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.5/act/Mul_output_0, %model.6.cv1.conv.weight, %model.6.cv1.conv.bias)\n",
      "  %/model.6/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.6/cv1/conv/Conv_output_0)\n",
      "  %/model.6/cv1/act/Mul_output_0 = Mul(%/model.6/cv1/conv/Conv_output_0, %/model.6/cv1/act/Sigmoid_output_0)\n",
      "  %/model.6/m/m.0/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.6/cv1/act/Mul_output_0, %model.6.m.0.cv1.conv.weight, %model.6.m.0.cv1.conv.bias)\n",
      "  %/model.6/m/m.0/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.6/m/m.0/cv1/conv/Conv_output_0)\n",
      "  %/model.6/m/m.0/cv1/act/Mul_output_0 = Mul(%/model.6/m/m.0/cv1/conv/Conv_output_0, %/model.6/m/m.0/cv1/act/Sigmoid_output_0)\n",
      "  %/model.6/m/m.0/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/model.6/m/m.0/cv1/act/Mul_output_0, %model.6.m.0.cv2.conv.weight, %model.6.m.0.cv2.conv.bias)\n",
      "  %/model.6/m/m.0/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.6/m/m.0/cv2/conv/Conv_output_0)\n",
      "  %/model.6/m/m.0/cv2/act/Mul_output_0 = Mul(%/model.6/m/m.0/cv2/conv/Conv_output_0, %/model.6/m/m.0/cv2/act/Sigmoid_output_0)\n",
      "  %/model.6/m/m.0/Add_output_0 = Add(%/model.6/cv1/act/Mul_output_0, %/model.6/m/m.0/cv2/act/Mul_output_0)\n",
      "  %/model.6/m/m.1/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.6/m/m.0/Add_output_0, %model.6.m.1.cv1.conv.weight, %model.6.m.1.cv1.conv.bias)\n",
      "  %/model.6/m/m.1/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.6/m/m.1/cv1/conv/Conv_output_0)\n",
      "  %/model.6/m/m.1/cv1/act/Mul_output_0 = Mul(%/model.6/m/m.1/cv1/conv/Conv_output_0, %/model.6/m/m.1/cv1/act/Sigmoid_output_0)\n",
      "  %/model.6/m/m.1/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/model.6/m/m.1/cv1/act/Mul_output_0, %model.6.m.1.cv2.conv.weight, %model.6.m.1.cv2.conv.bias)\n",
      "  %/model.6/m/m.1/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.6/m/m.1/cv2/conv/Conv_output_0)\n",
      "  %/model.6/m/m.1/cv2/act/Mul_output_0 = Mul(%/model.6/m/m.1/cv2/conv/Conv_output_0, %/model.6/m/m.1/cv2/act/Sigmoid_output_0)\n",
      "  %/model.6/m/m.1/Add_output_0 = Add(%/model.6/m/m.0/Add_output_0, %/model.6/m/m.1/cv2/act/Mul_output_0)\n",
      "  %/model.6/m/m.2/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.6/m/m.1/Add_output_0, %model.6.m.2.cv1.conv.weight, %model.6.m.2.cv1.conv.bias)\n",
      "  %/model.6/m/m.2/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.6/m/m.2/cv1/conv/Conv_output_0)\n",
      "  %/model.6/m/m.2/cv1/act/Mul_output_0 = Mul(%/model.6/m/m.2/cv1/conv/Conv_output_0, %/model.6/m/m.2/cv1/act/Sigmoid_output_0)\n",
      "  %/model.6/m/m.2/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/model.6/m/m.2/cv1/act/Mul_output_0, %model.6.m.2.cv2.conv.weight, %model.6.m.2.cv2.conv.bias)\n",
      "  %/model.6/m/m.2/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.6/m/m.2/cv2/conv/Conv_output_0)\n",
      "  %/model.6/m/m.2/cv2/act/Mul_output_0 = Mul(%/model.6/m/m.2/cv2/conv/Conv_output_0, %/model.6/m/m.2/cv2/act/Sigmoid_output_0)\n",
      "  %/model.6/m/m.2/Add_output_0 = Add(%/model.6/m/m.1/Add_output_0, %/model.6/m/m.2/cv2/act/Mul_output_0)\n",
      "  %/model.6/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.5/act/Mul_output_0, %model.6.cv2.conv.weight, %model.6.cv2.conv.bias)\n",
      "  %/model.6/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.6/cv2/conv/Conv_output_0)\n",
      "  %/model.6/cv2/act/Mul_output_0 = Mul(%/model.6/cv2/conv/Conv_output_0, %/model.6/cv2/act/Sigmoid_output_0)\n",
      "  %/model.6/Concat_output_0 = Concat[axis = 1](%/model.6/m/m.2/Add_output_0, %/model.6/cv2/act/Mul_output_0)\n",
      "  %/model.6/cv3/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.6/Concat_output_0, %model.6.cv3.conv.weight, %model.6.cv3.conv.bias)\n",
      "  %/model.6/cv3/act/Sigmoid_output_0 = Sigmoid(%/model.6/cv3/conv/Conv_output_0)\n",
      "  %/model.6/cv3/act/Mul_output_0 = Mul(%/model.6/cv3/conv/Conv_output_0, %/model.6/cv3/act/Sigmoid_output_0)\n",
      "  %/model.7/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%/model.6/cv3/act/Mul_output_0, %model.7.conv.weight, %model.7.conv.bias)\n",
      "  %/model.7/act/Sigmoid_output_0 = Sigmoid(%/model.7/conv/Conv_output_0)\n",
      "  %/model.7/act/Mul_output_0 = Mul(%/model.7/conv/Conv_output_0, %/model.7/act/Sigmoid_output_0)\n",
      "  %/model.8/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.7/act/Mul_output_0, %model.8.cv1.conv.weight, %model.8.cv1.conv.bias)\n",
      "  %/model.8/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.8/cv1/conv/Conv_output_0)\n",
      "  %/model.8/cv1/act/Mul_output_0 = Mul(%/model.8/cv1/conv/Conv_output_0, %/model.8/cv1/act/Sigmoid_output_0)\n",
      "  %/model.8/m/m.0/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.8/cv1/act/Mul_output_0, %model.8.m.0.cv1.conv.weight, %model.8.m.0.cv1.conv.bias)\n",
      "  %/model.8/m/m.0/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.8/m/m.0/cv1/conv/Conv_output_0)\n",
      "  %/model.8/m/m.0/cv1/act/Mul_output_0 = Mul(%/model.8/m/m.0/cv1/conv/Conv_output_0, %/model.8/m/m.0/cv1/act/Sigmoid_output_0)\n",
      "  %/model.8/m/m.0/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/model.8/m/m.0/cv1/act/Mul_output_0, %model.8.m.0.cv2.conv.weight, %model.8.m.0.cv2.conv.bias)\n",
      "  %/model.8/m/m.0/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.8/m/m.0/cv2/conv/Conv_output_0)\n",
      "  %/model.8/m/m.0/cv2/act/Mul_output_0 = Mul(%/model.8/m/m.0/cv2/conv/Conv_output_0, %/model.8/m/m.0/cv2/act/Sigmoid_output_0)\n",
      "  %/model.8/m/m.0/Add_output_0 = Add(%/model.8/cv1/act/Mul_output_0, %/model.8/m/m.0/cv2/act/Mul_output_0)\n",
      "  %/model.8/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.7/act/Mul_output_0, %model.8.cv2.conv.weight, %model.8.cv2.conv.bias)\n",
      "  %/model.8/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.8/cv2/conv/Conv_output_0)\n",
      "  %/model.8/cv2/act/Mul_output_0 = Mul(%/model.8/cv2/conv/Conv_output_0, %/model.8/cv2/act/Sigmoid_output_0)\n",
      "  %/model.8/Concat_output_0 = Concat[axis = 1](%/model.8/m/m.0/Add_output_0, %/model.8/cv2/act/Mul_output_0)\n",
      "  %/model.8/cv3/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.8/Concat_output_0, %model.8.cv3.conv.weight, %model.8.cv3.conv.bias)\n",
      "  %/model.8/cv3/act/Sigmoid_output_0 = Sigmoid(%/model.8/cv3/conv/Conv_output_0)\n",
      "  %/model.8/cv3/act/Mul_output_0 = Mul(%/model.8/cv3/conv/Conv_output_0, %/model.8/cv3/act/Sigmoid_output_0)\n",
      "  %/model.9/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.8/cv3/act/Mul_output_0, %model.9.cv1.conv.weight, %model.9.cv1.conv.bias)\n",
      "  %/model.9/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.9/cv1/conv/Conv_output_0)\n",
      "  %/model.9/cv1/act/Mul_output_0 = Mul(%/model.9/cv1/conv/Conv_output_0, %/model.9/cv1/act/Sigmoid_output_0)\n",
      "  %/model.9/m/MaxPool_output_0 = MaxPool[ceil_mode = 0, dilations = [1, 1], kernel_shape = [5, 5], pads = [2, 2, 2, 2], strides = [1, 1]](%/model.9/cv1/act/Mul_output_0)\n",
      "  %/model.9/m_1/MaxPool_output_0 = MaxPool[ceil_mode = 0, dilations = [1, 1], kernel_shape = [5, 5], pads = [2, 2, 2, 2], strides = [1, 1]](%/model.9/m/MaxPool_output_0)\n",
      "  %/model.9/m_2/MaxPool_output_0 = MaxPool[ceil_mode = 0, dilations = [1, 1], kernel_shape = [5, 5], pads = [2, 2, 2, 2], strides = [1, 1]](%/model.9/m_1/MaxPool_output_0)\n",
      "  %/model.9/Concat_output_0 = Concat[axis = 1](%/model.9/cv1/act/Mul_output_0, %/model.9/m/MaxPool_output_0, %/model.9/m_1/MaxPool_output_0, %/model.9/m_2/MaxPool_output_0)\n",
      "  %/model.9/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.9/Concat_output_0, %model.9.cv2.conv.weight, %model.9.cv2.conv.bias)\n",
      "  %/model.9/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.9/cv2/conv/Conv_output_0)\n",
      "  %/model.9/cv2/act/Mul_output_0 = Mul(%/model.9/cv2/conv/Conv_output_0, %/model.9/cv2/act/Sigmoid_output_0)\n",
      "  %/model.10/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.9/cv2/act/Mul_output_0, %model.10.conv.weight, %model.10.conv.bias)\n",
      "  %/model.10/act/Sigmoid_output_0 = Sigmoid(%/model.10/conv/Conv_output_0)\n",
      "  %/model.10/act/Mul_output_0 = Mul(%/model.10/conv/Conv_output_0, %/model.10/act/Sigmoid_output_0)\n",
      "  %/model.11/Resize_output_0 = Resize[coordinate_transformation_mode = 'asymmetric', cubic_coeff_a = -0.75, mode = 'nearest', nearest_mode = 'floor'](%/model.10/act/Mul_output_0, %/model.11/Constant_1_output_0, %/model.11/Constant_output_0)\n",
      "  %/model.12/Concat_output_0 = Concat[axis = 1](%/model.11/Resize_output_0, %/model.6/cv3/act/Mul_output_0)\n",
      "  %/model.13/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.12/Concat_output_0, %model.13.cv1.conv.weight, %model.13.cv1.conv.bias)\n",
      "  %/model.13/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.13/cv1/conv/Conv_output_0)\n",
      "  %/model.13/cv1/act/Mul_output_0 = Mul(%/model.13/cv1/conv/Conv_output_0, %/model.13/cv1/act/Sigmoid_output_0)\n",
      "  %/model.13/m/m.0/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.13/cv1/act/Mul_output_0, %model.13.m.0.cv1.conv.weight, %model.13.m.0.cv1.conv.bias)\n",
      "  %/model.13/m/m.0/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.13/m/m.0/cv1/conv/Conv_output_0)\n",
      "  %/model.13/m/m.0/cv1/act/Mul_output_0 = Mul(%/model.13/m/m.0/cv1/conv/Conv_output_0, %/model.13/m/m.0/cv1/act/Sigmoid_output_0)\n",
      "  %/model.13/m/m.0/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/model.13/m/m.0/cv1/act/Mul_output_0, %model.13.m.0.cv2.conv.weight, %model.13.m.0.cv2.conv.bias)\n",
      "  %/model.13/m/m.0/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.13/m/m.0/cv2/conv/Conv_output_0)\n",
      "  %/model.13/m/m.0/cv2/act/Mul_output_0 = Mul(%/model.13/m/m.0/cv2/conv/Conv_output_0, %/model.13/m/m.0/cv2/act/Sigmoid_output_0)\n",
      "  %/model.13/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.12/Concat_output_0, %model.13.cv2.conv.weight, %model.13.cv2.conv.bias)\n",
      "  %/model.13/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.13/cv2/conv/Conv_output_0)\n",
      "  %/model.13/cv2/act/Mul_output_0 = Mul(%/model.13/cv2/conv/Conv_output_0, %/model.13/cv2/act/Sigmoid_output_0)\n",
      "  %/model.13/Concat_output_0 = Concat[axis = 1](%/model.13/m/m.0/cv2/act/Mul_output_0, %/model.13/cv2/act/Mul_output_0)\n",
      "  %/model.13/cv3/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.13/Concat_output_0, %model.13.cv3.conv.weight, %model.13.cv3.conv.bias)\n",
      "  %/model.13/cv3/act/Sigmoid_output_0 = Sigmoid(%/model.13/cv3/conv/Conv_output_0)\n",
      "  %/model.13/cv3/act/Mul_output_0 = Mul(%/model.13/cv3/conv/Conv_output_0, %/model.13/cv3/act/Sigmoid_output_0)\n",
      "  %/model.14/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.13/cv3/act/Mul_output_0, %model.14.conv.weight, %model.14.conv.bias)\n",
      "  %/model.14/act/Sigmoid_output_0 = Sigmoid(%/model.14/conv/Conv_output_0)\n",
      "  %/model.14/act/Mul_output_0 = Mul(%/model.14/conv/Conv_output_0, %/model.14/act/Sigmoid_output_0)\n",
      "  %/model.15/Resize_output_0 = Resize[coordinate_transformation_mode = 'asymmetric', cubic_coeff_a = -0.75, mode = 'nearest', nearest_mode = 'floor'](%/model.14/act/Mul_output_0, %/model.11/Constant_1_output_0, %/model.11/Constant_output_0)\n",
      "  %/model.16/Concat_output_0 = Concat[axis = 1](%/model.15/Resize_output_0, %/model.4/cv3/act/Mul_output_0)\n",
      "  %/model.17/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.16/Concat_output_0, %model.17.cv1.conv.weight, %model.17.cv1.conv.bias)\n",
      "  %/model.17/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.17/cv1/conv/Conv_output_0)\n",
      "  %/model.17/cv1/act/Mul_output_0 = Mul(%/model.17/cv1/conv/Conv_output_0, %/model.17/cv1/act/Sigmoid_output_0)\n",
      "  %/model.17/m/m.0/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.17/cv1/act/Mul_output_0, %model.17.m.0.cv1.conv.weight, %model.17.m.0.cv1.conv.bias)\n",
      "  %/model.17/m/m.0/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.17/m/m.0/cv1/conv/Conv_output_0)\n",
      "  %/model.17/m/m.0/cv1/act/Mul_output_0 = Mul(%/model.17/m/m.0/cv1/conv/Conv_output_0, %/model.17/m/m.0/cv1/act/Sigmoid_output_0)\n",
      "  %/model.17/m/m.0/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/model.17/m/m.0/cv1/act/Mul_output_0, %model.17.m.0.cv2.conv.weight, %model.17.m.0.cv2.conv.bias)\n",
      "  %/model.17/m/m.0/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.17/m/m.0/cv2/conv/Conv_output_0)\n",
      "  %/model.17/m/m.0/cv2/act/Mul_output_0 = Mul(%/model.17/m/m.0/cv2/conv/Conv_output_0, %/model.17/m/m.0/cv2/act/Sigmoid_output_0)\n",
      "  %/model.17/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.16/Concat_output_0, %model.17.cv2.conv.weight, %model.17.cv2.conv.bias)\n",
      "  %/model.17/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.17/cv2/conv/Conv_output_0)\n",
      "  %/model.17/cv2/act/Mul_output_0 = Mul(%/model.17/cv2/conv/Conv_output_0, %/model.17/cv2/act/Sigmoid_output_0)\n",
      "  %/model.17/Concat_output_0 = Concat[axis = 1](%/model.17/m/m.0/cv2/act/Mul_output_0, %/model.17/cv2/act/Mul_output_0)\n",
      "  %/model.17/cv3/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.17/Concat_output_0, %model.17.cv3.conv.weight, %model.17.cv3.conv.bias)\n",
      "  %/model.17/cv3/act/Sigmoid_output_0 = Sigmoid(%/model.17/cv3/conv/Conv_output_0)\n",
      "  %/model.17/cv3/act/Mul_output_0 = Mul(%/model.17/cv3/conv/Conv_output_0, %/model.17/cv3/act/Sigmoid_output_0)\n",
      "  %/model.18/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%/model.17/cv3/act/Mul_output_0, %model.18.conv.weight, %model.18.conv.bias)\n",
      "  %/model.18/act/Sigmoid_output_0 = Sigmoid(%/model.18/conv/Conv_output_0)\n",
      "  %/model.18/act/Mul_output_0 = Mul(%/model.18/conv/Conv_output_0, %/model.18/act/Sigmoid_output_0)\n",
      "  %/model.19/Concat_output_0 = Concat[axis = 1](%/model.18/act/Mul_output_0, %/model.14/act/Mul_output_0)\n",
      "  %/model.20/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.19/Concat_output_0, %model.20.cv1.conv.weight, %model.20.cv1.conv.bias)\n",
      "  %/model.20/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.20/cv1/conv/Conv_output_0)\n",
      "  %/model.20/cv1/act/Mul_output_0 = Mul(%/model.20/cv1/conv/Conv_output_0, %/model.20/cv1/act/Sigmoid_output_0)\n",
      "  %/model.20/m/m.0/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.20/cv1/act/Mul_output_0, %model.20.m.0.cv1.conv.weight, %model.20.m.0.cv1.conv.bias)\n",
      "  %/model.20/m/m.0/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.20/m/m.0/cv1/conv/Conv_output_0)\n",
      "  %/model.20/m/m.0/cv1/act/Mul_output_0 = Mul(%/model.20/m/m.0/cv1/conv/Conv_output_0, %/model.20/m/m.0/cv1/act/Sigmoid_output_0)\n",
      "  %/model.20/m/m.0/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/model.20/m/m.0/cv1/act/Mul_output_0, %model.20.m.0.cv2.conv.weight, %model.20.m.0.cv2.conv.bias)\n",
      "  %/model.20/m/m.0/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.20/m/m.0/cv2/conv/Conv_output_0)\n",
      "  %/model.20/m/m.0/cv2/act/Mul_output_0 = Mul(%/model.20/m/m.0/cv2/conv/Conv_output_0, %/model.20/m/m.0/cv2/act/Sigmoid_output_0)\n",
      "  %/model.20/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.19/Concat_output_0, %model.20.cv2.conv.weight, %model.20.cv2.conv.bias)\n",
      "  %/model.20/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.20/cv2/conv/Conv_output_0)\n",
      "  %/model.20/cv2/act/Mul_output_0 = Mul(%/model.20/cv2/conv/Conv_output_0, %/model.20/cv2/act/Sigmoid_output_0)\n",
      "  %/model.20/Concat_output_0 = Concat[axis = 1](%/model.20/m/m.0/cv2/act/Mul_output_0, %/model.20/cv2/act/Mul_output_0)\n",
      "  %/model.20/cv3/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.20/Concat_output_0, %model.20.cv3.conv.weight, %model.20.cv3.conv.bias)\n",
      "  %/model.20/cv3/act/Sigmoid_output_0 = Sigmoid(%/model.20/cv3/conv/Conv_output_0)\n",
      "  %/model.20/cv3/act/Mul_output_0 = Mul(%/model.20/cv3/conv/Conv_output_0, %/model.20/cv3/act/Sigmoid_output_0)\n",
      "  %/model.21/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%/model.20/cv3/act/Mul_output_0, %model.21.conv.weight, %model.21.conv.bias)\n",
      "  %/model.21/act/Sigmoid_output_0 = Sigmoid(%/model.21/conv/Conv_output_0)\n",
      "  %/model.21/act/Mul_output_0 = Mul(%/model.21/conv/Conv_output_0, %/model.21/act/Sigmoid_output_0)\n",
      "  %/model.22/Concat_output_0 = Concat[axis = 1](%/model.21/act/Mul_output_0, %/model.10/act/Mul_output_0)\n",
      "  %/model.23/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.22/Concat_output_0, %model.23.cv1.conv.weight, %model.23.cv1.conv.bias)\n",
      "  %/model.23/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.23/cv1/conv/Conv_output_0)\n",
      "  %/model.23/cv1/act/Mul_output_0 = Mul(%/model.23/cv1/conv/Conv_output_0, %/model.23/cv1/act/Sigmoid_output_0)\n",
      "  %/model.23/m/m.0/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.23/cv1/act/Mul_output_0, %model.23.m.0.cv1.conv.weight, %model.23.m.0.cv1.conv.bias)\n",
      "  %/model.23/m/m.0/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.23/m/m.0/cv1/conv/Conv_output_0)\n",
      "  %/model.23/m/m.0/cv1/act/Mul_output_0 = Mul(%/model.23/m/m.0/cv1/conv/Conv_output_0, %/model.23/m/m.0/cv1/act/Sigmoid_output_0)\n",
      "  %/model.23/m/m.0/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/model.23/m/m.0/cv1/act/Mul_output_0, %model.23.m.0.cv2.conv.weight, %model.23.m.0.cv2.conv.bias)\n",
      "  %/model.23/m/m.0/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.23/m/m.0/cv2/conv/Conv_output_0)\n",
      "  %/model.23/m/m.0/cv2/act/Mul_output_0 = Mul(%/model.23/m/m.0/cv2/conv/Conv_output_0, %/model.23/m/m.0/cv2/act/Sigmoid_output_0)\n",
      "  %/model.23/cv2/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.22/Concat_output_0, %model.23.cv2.conv.weight, %model.23.cv2.conv.bias)\n",
      "  %/model.23/cv2/act/Sigmoid_output_0 = Sigmoid(%/model.23/cv2/conv/Conv_output_0)\n",
      "  %/model.23/cv2/act/Mul_output_0 = Mul(%/model.23/cv2/conv/Conv_output_0, %/model.23/cv2/act/Sigmoid_output_0)\n",
      "  %/model.23/Concat_output_0 = Concat[axis = 1](%/model.23/m/m.0/cv2/act/Mul_output_0, %/model.23/cv2/act/Mul_output_0)\n",
      "  %/model.23/cv3/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.23/Concat_output_0, %model.23.cv3.conv.weight, %model.23.cv3.conv.bias)\n",
      "  %/model.23/cv3/act/Sigmoid_output_0 = Sigmoid(%/model.23/cv3/conv/Conv_output_0)\n",
      "  %/model.23/cv3/act/Mul_output_0 = Mul(%/model.23/cv3/conv/Conv_output_0, %/model.23/cv3/act/Sigmoid_output_0)\n",
      "  %/model.24/m.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.17/cv3/act/Mul_output_0, %model.24.m.0.weight, %model.24.m.0.bias)\n",
      "  %/model.24/Reshape_output_0 = Reshape(%/model.24/m.0/Conv_output_0, %/model.24/Constant_output_0)\n",
      "  %/model.24/Transpose_output_0 = Transpose[perm = [0, 1, 3, 4, 2]](%/model.24/Reshape_output_0)\n",
      "  %/model.24/Sigmoid_output_0 = Sigmoid(%/model.24/Transpose_output_0)\n",
      "  %/model.24/Split_output_0, %/model.24/Split_output_1, %/model.24/Split_output_2 = Split[axis = 4, split = [2, 2, 15]](%/model.24/Sigmoid_output_0)\n",
      "  %/model.24/Mul_output_0 = Mul(%/model.24/Split_output_0, %/model.24/Constant_1_output_0)\n",
      "  %/model.24/Add_output_0 = Add(%/model.24/Mul_output_0, %/model.24/Constant_2_output_0)\n",
      "  %/model.24/Mul_1_output_0 = Mul(%/model.24/Add_output_0, %/model.24/Constant_3_output_0)\n",
      "  %/model.24/Mul_2_output_0 = Mul(%/model.24/Split_output_1, %/model.24/Constant_1_output_0)\n",
      "  %/model.24/Pow_output_0 = Pow(%/model.24/Mul_2_output_0, %/model.24/Constant_1_output_0)\n",
      "  %/model.24/Mul_3_output_0 = Mul(%/model.24/Pow_output_0, %/model.24/Constant_6_output_0)\n",
      "  %/model.24/Concat_output_0 = Concat[axis = 4](%/model.24/Mul_1_output_0, %/model.24/Mul_3_output_0, %/model.24/Split_output_2)\n",
      "  %/model.24/Reshape_1_output_0 = Reshape(%/model.24/Concat_output_0, %/model.24/Constant_7_output_0)\n",
      "  %/model.24/m.1/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.20/cv3/act/Mul_output_0, %model.24.m.1.weight, %model.24.m.1.bias)\n",
      "  %/model.24/Reshape_2_output_0 = Reshape(%/model.24/m.1/Conv_output_0, %/model.24/Constant_8_output_0)\n",
      "  %/model.24/Transpose_1_output_0 = Transpose[perm = [0, 1, 3, 4, 2]](%/model.24/Reshape_2_output_0)\n",
      "  %/model.24/Sigmoid_1_output_0 = Sigmoid(%/model.24/Transpose_1_output_0)\n",
      "  %/model.24/Split_1_output_0, %/model.24/Split_1_output_1, %/model.24/Split_1_output_2 = Split[axis = 4, split = [2, 2, 15]](%/model.24/Sigmoid_1_output_0)\n",
      "  %/model.24/Mul_4_output_0 = Mul(%/model.24/Split_1_output_0, %/model.24/Constant_1_output_0)\n",
      "  %/model.24/Add_1_output_0 = Add(%/model.24/Mul_4_output_0, %/model.24/Constant_10_output_0)\n",
      "  %/model.24/Mul_5_output_0 = Mul(%/model.24/Add_1_output_0, %/model.24/Constant_11_output_0)\n",
      "  %/model.24/Mul_6_output_0 = Mul(%/model.24/Split_1_output_1, %/model.24/Constant_1_output_0)\n",
      "  %/model.24/Pow_1_output_0 = Pow(%/model.24/Mul_6_output_0, %/model.24/Constant_1_output_0)\n",
      "  %/model.24/Mul_7_output_0 = Mul(%/model.24/Pow_1_output_0, %/model.24/Constant_14_output_0)\n",
      "  %/model.24/Concat_1_output_0 = Concat[axis = 4](%/model.24/Mul_5_output_0, %/model.24/Mul_7_output_0, %/model.24/Split_1_output_2)\n",
      "  %/model.24/Reshape_3_output_0 = Reshape(%/model.24/Concat_1_output_0, %/model.24/Constant_15_output_0)\n",
      "  %/model.24/m.2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.23/cv3/act/Mul_output_0, %model.24.m.2.weight, %model.24.m.2.bias)\n",
      "  %/model.24/Reshape_4_output_0 = Reshape(%/model.24/m.2/Conv_output_0, %/model.24/Constant_16_output_0)\n",
      "  %/model.24/Transpose_2_output_0 = Transpose[perm = [0, 1, 3, 4, 2]](%/model.24/Reshape_4_output_0)\n",
      "  %/model.24/Sigmoid_2_output_0 = Sigmoid(%/model.24/Transpose_2_output_0)\n",
      "  %/model.24/Split_2_output_0, %/model.24/Split_2_output_1, %/model.24/Split_2_output_2 = Split[axis = 4, split = [2, 2, 15]](%/model.24/Sigmoid_2_output_0)\n",
      "  %/model.24/Mul_8_output_0 = Mul(%/model.24/Split_2_output_0, %/model.24/Constant_1_output_0)\n",
      "  %/model.24/Add_2_output_0 = Add(%/model.24/Mul_8_output_0, %/model.24/Constant_18_output_0)\n",
      "  %/model.24/Mul_9_output_0 = Mul(%/model.24/Add_2_output_0, %/model.24/Constant_19_output_0)\n",
      "  %/model.24/Mul_10_output_0 = Mul(%/model.24/Split_2_output_1, %/model.24/Constant_1_output_0)\n",
      "  %/model.24/Pow_2_output_0 = Pow(%/model.24/Mul_10_output_0, %/model.24/Constant_1_output_0)\n",
      "  %/model.24/Mul_11_output_0 = Mul(%/model.24/Pow_2_output_0, %/model.24/Constant_22_output_0)\n",
      "  %/model.24/Concat_2_output_0 = Concat[axis = 4](%/model.24/Mul_9_output_0, %/model.24/Mul_11_output_0, %/model.24/Split_2_output_2)\n",
      "  %/model.24/Reshape_5_output_0 = Reshape(%/model.24/Concat_2_output_0, %/model.24/Constant_23_output_0)\n",
      "  %output0 = Concat[axis = 1](%/model.24/Reshape_1_output_0, %/model.24/Reshape_3_output_0, %/model.24/Reshape_5_output_0)\n",
      "  return %output0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the YOLO model in ONNX format\n",
    "model = onnx.load('C:/Users/Gnana Bharathi S/Downloads/Feb23_Police/Feb23_Police.onnx')\n",
    "\n",
    "# Print model graph to visualize the architecture\n",
    "print(onnx.helper.printable_graph(model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: netron in c:\\users\\gnana bharathi s\\anaconda3\\lib\\site-packages (7.5.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Knife', 1: 'Handgun'}\n",
      "['badge', 'baton', 'duty belt', 'handcuffs', 'lights', 'nameplate', 'officer', 'patch', 'police hat', 'radio', 'sunglasses', 'text', 'vehicle', 'watch']\n",
      "End of video stream\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import cv2\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "import time\n",
    "\n",
    "with open(\"C:/Users/Gnana Bharathi S/Downloads/dataset/dataset.yaml\", mode='r') as f:\n",
    "    data_yaml = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "labels = data_yaml['names']\n",
    "print(labels)\n",
    "\n",
    "\n",
    "\n",
    "with open(\"C:/Users/Gnana Bharathi S/Downloads/Feb23_Police/data_1.yaml\", mode='r') as f:\n",
    "    pol_yaml = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "labels1 = pol_yaml['names']\n",
    "print(labels1)\n",
    "\n",
    "# Load YOLO model for gun detection\n",
    "yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/Dec 14/C:/Users/Gnana Bharathi S/Downloads/Weapon Model/best.onnx')\n",
    "#yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/Dec 14/knife.onnx')\n",
    "#yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/feb3.onnx')\n",
    "yolo.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "yolo.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "# Load emotion detection model\n",
    "model = DeepFace.build_model(\"Emotion\")\n",
    "\n",
    "# Define emotion labels\n",
    "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "\n",
    "# Load face cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Open video capture\n",
    "#video_path = 'C:/Users/Gnana Bharathi S/Downloads/demo2.mp4'  # Update with your video file path\n",
    "#video_path='C:/Users/Gnana Bharathi S/Downloads/feb8vid1.mp4'\n",
    "video_path='C:/Users/Gnana Bharathi S/Downloads/pol3.mp4'\n",
    "#video_path=\"C:/Users/Gnana Bharathi S/Videos/20240313_110450.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "#cap = cv2.VideoCapture(0)\n",
    "start_time = None\n",
    "\n",
    "\n",
    "'''yolo_knife = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/Dec 14/knife.onnx')\n",
    "#yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/feb3.onnx')\n",
    "yolo_knife.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "yolo_knife.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)'''\n",
    "\n",
    "frame_skip = 5  # Process every 5th frame\n",
    "frame_count = 0\n",
    "\n",
    "#Police\n",
    "yolo_p = cv2.dnn.readNetFromONNX(\"C:/Users/Gnana Bharathi S/Downloads/Feb23_Police/Feb23_Police.onnx\")\n",
    "yolo_p.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "yolo_p.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "gun=2\n",
    "knife=0\n",
    "em=0\n",
    "emotion=''\n",
    "wep=2\n",
    "pol=0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video stream\")\n",
    "        break\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gun, Knife, Police"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gun', 'knife']\n",
      "['badge', 'baton', 'duty belt', 'handcuffs', 'lights', 'nameplate', 'officer', 'patch', 'police hat', 'radio', 'sunglasses', 'text', 'vehicle', 'watch']\n",
      "police =  officer:87%\n",
      "police =  officer:81%\n",
      "()\n",
      "police =  officer:87%\n",
      "police =  officer:80%\n",
      "()\n",
      "police =  officer:87%\n",
      "police =  officer:80%\n",
      "()\n",
      "Safe since police is present\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import cv2\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "import time\n",
    "\n",
    "with open(\"C:/Users/Gnana Bharathi S/Downloads/Gun_Knife/weights/names.yaml\", mode='r') as f:\n",
    "    data_yaml = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "labels = data_yaml['names']\n",
    "print(labels)\n",
    "\n",
    "\n",
    "\n",
    "with open(\"C:/Users/Gnana Bharathi S/Downloads/Feb23_Police/data_1.yaml\", mode='r') as f:\n",
    "    pol_yaml = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "labels1 = pol_yaml['names']\n",
    "print(labels1)\n",
    "\n",
    "# Load YOLO model for gun detection\n",
    "yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/Weapon Model/best.onnx')\n",
    "#yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/Dec 14/knife.onnx')\n",
    "#yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/feb3.onnx')\n",
    "yolo.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "yolo.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "# Load emotion detection model\n",
    "model = DeepFace.build_model(\"Emotion\")\n",
    "\n",
    "# Define emotion labels\n",
    "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "\n",
    "# Load face cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Open video capture\n",
    "#video_path = 'C:/Users/Gnana Bharathi S/Downloads/demo2.mp4'  # Update with your video file path\n",
    "#video_path='C:/Users/Gnana Bharathi S/Downloads/feb8vid3.mp4'\n",
    "video_path='C:/Users/Gnana Bharathi S/Downloads/pol4.mp4'\n",
    "#video_path=\"C:/Users/Gnana Bharathi S/Videos/20240313_110450.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "#cap = cv2.VideoCapture(0)\n",
    "start_time = None\n",
    "\n",
    "\n",
    "'''yolo_knife = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/Dec 14/knife.onnx')\n",
    "#yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/feb3.onnx')\n",
    "yolo_knife.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "yolo_knife.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)'''\n",
    "frame_skip = 5  # Process every 5th frame\n",
    "frame_count = 0\n",
    "\n",
    "#Police\n",
    "yolo_p = cv2.dnn.readNetFromONNX(\"C:/Users/Gnana Bharathi S/Downloads/Feb23_Police/Feb23_Police.onnx\")\n",
    "yolo_p.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "yolo_p.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "gun=2\n",
    "knife=0\n",
    "em=0\n",
    "emotion=''\n",
    "wep=2\n",
    "pol=0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video stream\")\n",
    "        break\n",
    "    #frame_count += 1\n",
    "    #if frame_count % frame_skip != 0:\n",
    "     #   continue\n",
    "\n",
    "    image = frame.copy()\n",
    "    resize_width = 480\n",
    "    resize_height = 480\n",
    "    input_image = cv2.resize(image, (resize_width, resize_height))\n",
    "    row, col, d = image.shape  # height, width, depth\n",
    "    max_rc = max(row, col)\n",
    "    input_image = np.zeros((max_rc, max_rc, 3), dtype=np.uint8)\n",
    "    \n",
    "    input_image[0:row, 0:col] = image\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    INPUT_WH_YOLO = 640\n",
    "    blob = cv2.dnn.blobFromImage(input_image, 1 / 255, (INPUT_WH_YOLO, INPUT_WH_YOLO), swapRB=True, crop=False)\n",
    "    yolo_p.setInput(blob)\n",
    "    preds = yolo_p.forward()\n",
    "\n",
    "    detections = preds[0]\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classes = []\n",
    "\n",
    "    image_w, image_h = input_image.shape[:2]\n",
    "    x_fac = image_w / INPUT_WH_YOLO\n",
    "    y_fac = image_h / INPUT_WH_YOLO\n",
    "\n",
    "    for i in range(len(detections)):\n",
    "        row = detections[i]\n",
    "        confidence = row[4]\n",
    "        if confidence > 0:\n",
    "            class_score = row[5:].max()\n",
    "            class_id = row[5:].argmax()\n",
    "            if class_score > 0.25:\n",
    "                cx, cy, w, h = row[0:4]\n",
    "                left = int((cx - 0.5 * w) * x_fac)\n",
    "                top = int((cy - 0.5 * h) * y_fac)\n",
    "                width = int(w * x_fac)\n",
    "                height = int(h * y_fac)\n",
    "                box = np.array([left, top, width, height])\n",
    "                confidences.append(confidence)\n",
    "                boxes.append(box)\n",
    "                classes.append(class_id)\n",
    "\n",
    "    boxes_np = np.array(boxes).tolist()\n",
    "    confidences_np = np.array(confidences).tolist()\n",
    "\n",
    "    result = cv2.dnn.NMSBoxes(boxes_np, confidences_np, 0.3, 0.2)\n",
    "\n",
    "    if len(result) > 0:\n",
    "        flattened_result = result.flatten()\n",
    "        for ind in flattened_result:\n",
    "            x, y, w, h = boxes_np[ind]\n",
    "            bb_conf = int(confidences_np[ind] * 100)\n",
    "            classes_id = classes[ind]\n",
    "            class_name = labels1[classes_id]\n",
    "            text = f'{class_name}:{bb_conf}%'\n",
    "            if(('officer' in class_name.lower() or 'text' in class_name.lower()) and bb_conf>70):\n",
    "                cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.putText(image, text, (x, y+20), cv2.FONT_HERSHEY_PLAIN, 0.7, (255, 255, 255), 1)\n",
    "                print('police = ',text)\n",
    "                pol=1\n",
    "            #else:\n",
    "             #   pol=0\n",
    "            #if('officer' in class_name.lower() or 'text' in class_name.lower()):\n",
    "             #   pol=1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    INPUT_WH_YOLO = 640\n",
    "    blob = cv2.dnn.blobFromImage(input_image, 1/255, (INPUT_WH_YOLO, INPUT_WH_YOLO), swapRB=True, crop=False)\n",
    "    yolo.setInput(blob)\n",
    "    preds = yolo.forward()\n",
    "\n",
    "    detections = preds[0]\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classes = []\n",
    "\n",
    "    image_w, image_h = input_image.shape[:2]\n",
    "    x_fac = image_w / INPUT_WH_YOLO\n",
    "    y_fac = image_h / INPUT_WH_YOLO\n",
    "\n",
    "    for i in range(len(detections)):\n",
    "        row = detections[i]\n",
    "        confidence = row[4]\n",
    "        if confidence > 0:\n",
    "            class_score = row[5:].max()\n",
    "            class_id = row[5:].argmax()\n",
    "            if class_score > 0.25:\n",
    "                cx, cy, w, h = row[0:4]\n",
    "                left = int((cx - 0.5 * w) * x_fac)\n",
    "                top = int((cy - 0.5 * h) * y_fac)\n",
    "                width = int(w * x_fac)\n",
    "                height = int(h * y_fac)\n",
    "                box = np.array([left, top, width, height])\n",
    "                confidences.append(confidence)\n",
    "                boxes.append(box)\n",
    "                classes.append(class_id)\n",
    "\n",
    "    boxes_np = np.array(boxes).tolist()\n",
    "    confidences_np = np.array(confidences).tolist()\n",
    "\n",
    "    result = cv2.dnn.NMSBoxes(boxes_np, confidences_np, 0.3, 0.2)\n",
    "    print(result)\n",
    "    if len(result) > 0:\n",
    "        flattened_result = result.flatten()\n",
    "        gun=1\n",
    "        wep=0\n",
    "        for ind in flattened_result:\n",
    "            x, y, w, h = boxes_np[ind]\n",
    "            bb_conf = int(confidences_np[ind] * 100)\n",
    "            classes_id = classes[ind]\n",
    "            class_name = labels[classes_id]\n",
    "            print(class_name)\n",
    "            if('gun' in class_name or 'handgun' in class_name):\n",
    "            # Draw a rectangle around the gun and label it as 'gun'\n",
    "                print(bb_conf)\n",
    "                if(bb_conf>0):\n",
    "                    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "                    cv2.putText(image, 'Gun', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 255), 1)\n",
    "                    #cv2.putText(image, f'Gun {bb_conf}% confidence', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 255), 1)\n",
    "            elif('knife' in class_name):\n",
    "                if(bb_conf>0):\n",
    "                    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "                    cv2.putText(image, 'Knife', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 255), 1)\n",
    "                    #cv2.putText(image, f'Gun {bb_conf}% confidence', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 255), 1)\n",
    "                \n",
    "    '''else:\n",
    "        yolo_knife.setInput(blob)\n",
    "        preds = yolo_knife.forward()\n",
    "\n",
    "        detections = preds[0]\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        classes = []\n",
    "\n",
    "        image_w, image_h = input_image.shape[:2]\n",
    "        x_fac = image_w / INPUT_WH_YOLO\n",
    "        y_fac = image_h / INPUT_WH_YOLO\n",
    "\n",
    "        for i in range(len(detections)):\n",
    "            row = detections[i]\n",
    "            confidence = row[4]\n",
    "            if confidence > 0:\n",
    "                class_score = row[5:].max()\n",
    "                class_id = row[5:].argmax()\n",
    "                if class_score > 0.25:\n",
    "                    cx, cy, w, h = row[0:4]\n",
    "                    left = int((cx - 0.5 * w) * x_fac)\n",
    "                    top = int((cy - 0.5 * h) * y_fac)\n",
    "                    width = int(w * x_fac)\n",
    "                    height = int(h * y_fac)\n",
    "                    box = np.array([left, top, width, height])\n",
    "                    confidences.append(confidence)\n",
    "                    boxes.append(box)\n",
    "                    classes.append(class_id)\n",
    "\n",
    "        boxes_np = np.array(boxes).tolist()\n",
    "        confidences_np = np.array(confidences).tolist()\n",
    "\n",
    "        result = cv2.dnn.NMSBoxes(boxes_np, confidences_np, 0.3, 0.2)\n",
    "        print(result)\n",
    "        if len(result) > 0:\n",
    "            flattened_result = result.flatten()\n",
    "            knife=1\n",
    "            wep=1\n",
    "            for ind in flattened_result:\n",
    "                x, y, w, h = boxes_np[ind]\n",
    "                bb_conf = int(confidences_np[ind] * 100)\n",
    "                classes_id = classes[ind]\n",
    "                class_name = labels[classes_id]\n",
    "\n",
    "                # Draw a rectangle around the gun and label it as 'gun'\n",
    "                print('bb_conf = ',bb_conf)\n",
    "                cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "                cv2.putText(image, 'Knife', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 255), 1)\n",
    "                #cv2.putText(image, f'Knife {bb_conf}% confidence', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 255), 1)'''\n",
    "\n",
    "    # Detect emotion of the person in the frame\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_roi = gray_frame[y:y + h, x:x + w]\n",
    "        resized_face = cv2.resize(face_roi, (48, 48), interpolation=cv2.INTER_AREA)\n",
    "        normalized_face = resized_face / 255.0\n",
    "        reshaped_face = normalized_face.reshape(1, 48, 48, 1)\n",
    "        preds = model.predict(reshaped_face)[0]\n",
    "        emotion_idx = preds.argmax()\n",
    "        emotion = emotion_labels[emotion_idx]\n",
    "        print(emotion)\n",
    "        confidence_percentage = preds[emotion_idx] * 100\n",
    "        # If gun is detected and emotion is sad or angry, stop running the camera after 3 seconds\n",
    "        if start_time is None:\n",
    "            start_time = time.time()  # Store the current time\n",
    "        else:\n",
    "            current_time = time.time()  # Get the current time\n",
    "            elapsed_time = current_time - start_time  # Calculate the elapsed time\n",
    "            if elapsed_time > 1000:  # Check if 5 seconds have passed\n",
    "                print(emotion)\n",
    "                cap.release()  # Release the camera\n",
    "                cv2.destroyAllWindows()  # Close the windows\n",
    "                break  # Break out of the while loop\n",
    "\n",
    "        # Draw a rectangle around the emotion and label it with predicted emotion\n",
    "        color = (0,0,255) if emotion == 'angry' else (0, 0, 255)\n",
    "        text = emotion\n",
    "        #text = f'{emotion}: {confidence_percentage:.2f}%'\n",
    "        if(confidence_percentage<80):\n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), color, 2)\n",
    "            cv2.putText(image, text, (x, y+100), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "    cv2.imshow(\"Video\", image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "if(pol==0):\n",
    "    from datetime import datetime\n",
    "    current_time = datetime.now()\n",
    "    hours = current_time.hour\n",
    "    minutes = current_time.minute\n",
    "    seconds = current_time.second\n",
    "    day=month=current_time.day\n",
    "    month=current_time.month\n",
    "    year=current_time.year\n",
    "\n",
    "    # Print the extracted values\n",
    "    #print(f\"Current Time: {current_time}\")\n",
    "    #print(f\"Hours: {hours}\")\n",
    "    #print(f\"Minutes: {minutes}\")\n",
    "    #print(f\"Seconds: {seconds}\")\n",
    "    #print(f\"Day: {day}\")\n",
    "    #print(f\"Month: {month}\")\n",
    "    #print(f\"Year: {year}\")\n",
    "\n",
    "    result = get_lat_long_by_ip()\n",
    "    latitude, longitude = result\n",
    "    #latitude = 41\n",
    "    #longitude=-87\n",
    "    #print('latitude = ',latitude)\n",
    "    #print('longitude = ',longitude)\n",
    "    x, y = convert_lat_lon_to_xy(latitude, longitude)\n",
    "    #x=1182280\n",
    "    #y=1881621\n",
    "    #print(f\"X Coordinate: {x}, Y Coordinate: {y}\")\n",
    "\n",
    "    if(emotion=='angry'):\n",
    "        em=0\n",
    "    elif(emotion=='disgust'):\n",
    "        em=1\n",
    "    elif(emotion=='fear'):\n",
    "        em=2\n",
    "    elif(emotion=='happy'):\n",
    "        em=3\n",
    "    elif(emotion=='neutral'):\n",
    "        em=4\n",
    "    elif(emotion=='sad'):\n",
    "        em=5\n",
    "    elif(emotion=='surprise'):\n",
    "        em=6\n",
    "    else:\n",
    "        em=4\n",
    "    # Replace these values with your own data\n",
    "    new_data = np.array([[wep,em,latitude, longitude, x, y, year, month, day, hours, minutes,seconds]])\n",
    "    #new_data = np.array([[1,2,41.830482, -87.621752, 1182280, 1881621, 2023, 12, 15, 14, 30,0]])\n",
    "    #new_data=np.array([[0,1,41.747610,-87.549179,1198234.0,1851595.0,2020,7,1,10,16,0]])\n",
    "    # Standardize the new data using the same scaler\n",
    "    #new_data_standardized = scaler.transform(new_data)\n",
    "\n",
    "    # Make predictions for the new data\n",
    "    new_data_prediction = loaded_model.predict(new_data)\n",
    "\n",
    "    print(f'Predicted Alert for the new data: {new_data_prediction[0]}')\n",
    "else:\n",
    "    print('Safe since police is present')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gun', 'knife']\n",
      "()\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "neutral\n",
      "()\n",
      "[24722]\n",
      "[24493 24314]\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "angry\n",
      "()\n",
      "()\n",
      "[24336]\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "angry\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "[25083]\n",
      "[24076 24184]\n",
      "[24211]\n",
      "Current Time: 2024-04-04 22:22:05.178202\n",
      "Hours: 22\n",
      "Minutes: 22\n",
      "Seconds: 5\n",
      "Day: 4\n",
      "Month: 4\n",
      "Year: 2024\n",
      "latitude =  41.0878\n",
      "longitude =  -87.2785\n",
      "X Coordinate: -9704926.40514712, Y Coordinate: 5019677.577207601\n",
      "Predicted Alert for the new data: Danger\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import cv2\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "import time\n",
    "\n",
    "with open(\"C:/Users/Gnana Bharathi S/Downloads/Gun_Knife/weights/names.yaml\", mode='r') as f:\n",
    "    data_yaml = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "labels = data_yaml['names']\n",
    "print(labels)\n",
    "# Load YOLO model for gun detection\n",
    "#yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/Dec 14/gun_knife_jan_30.onnx')\n",
    "yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/Gun_Knifea03/weights/best.onnx')\n",
    "#yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/feb3.onnx')\n",
    "yolo.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "yolo.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "# Load emotion detection model\n",
    "model = DeepFace.build_model(\"Emotion\")\n",
    "\n",
    "# Define emotion labels\n",
    "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "\n",
    "# Load face cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Open video capture\n",
    "#video_path = 'C:/Users/Gnana Bharathi S/Downloads/demo2.mp4'  # Update with your video file path\n",
    "video_path='C:/Users/Gnana Bharathi S/Downloads/pol3.mp4'\n",
    "video_path='C:/Users/Gnana Bharathi S/Downloads/feb8vid.mp4'\n",
    "#cap = cv2.VideoCapture(video_path)\n",
    "cap = cv2.VideoCapture(0)\n",
    "start_time = None\n",
    "\n",
    "yolo_p = cv2.dnn.readNetFromONNX(\"C:/Users/Gnana Bharathi S/Downloads/Feb23_Police/Feb23_Police.onnx\")\n",
    "yolo_p.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "yolo_p.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "\n",
    "'''yolo_knife = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/Dec 14/knife.onnx')\n",
    "#yolo = cv2.dnn.readNetFromONNX('C:/Users/Gnana Bharathi S/Downloads/feb3.onnx')\n",
    "yolo_knife.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "yolo_knife.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "'''\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video stream\")\n",
    "        break\n",
    "\n",
    "    image = frame.copy()\n",
    "\n",
    "    row, col, d = image.shape  # height, width, depth\n",
    "    max_rc = max(row, col)\n",
    "    input_image = np.zeros((max_rc, max_rc, 3), dtype=np.uint8)\n",
    "    input_image[0:row, 0:col] = image\n",
    "    \n",
    "    \n",
    "    INPUT_WH_YOLO = 640\n",
    "    blob = cv2.dnn.blobFromImage(input_image, 1 / 255, (INPUT_WH_YOLO, INPUT_WH_YOLO), swapRB=True, crop=False)\n",
    "    yolo_p.setInput(blob)\n",
    "    preds = yolo_p.forward()\n",
    "\n",
    "    detections = preds[0]\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classes = []\n",
    "\n",
    "    image_w, image_h = input_image.shape[:2]\n",
    "    x_fac = image_w / INPUT_WH_YOLO\n",
    "    y_fac = image_h / INPUT_WH_YOLO\n",
    "\n",
    "    for i in range(len(detections)):\n",
    "        row = detections[i]\n",
    "        confidence = row[4]\n",
    "        if confidence > 0:\n",
    "            class_score = row[5:].max()\n",
    "            class_id = row[5:].argmax()\n",
    "            if class_score > 0.25:\n",
    "                cx, cy, w, h = row[0:4]\n",
    "                left = int((cx - 0.5 * w) * x_fac)\n",
    "                top = int((cy - 0.5 * h) * y_fac)\n",
    "                width = int(w * x_fac)\n",
    "                height = int(h * y_fac)\n",
    "                box = np.array([left, top, width, height])\n",
    "                confidences.append(confidence)\n",
    "                boxes.append(box)\n",
    "                classes.append(class_id)\n",
    "\n",
    "    boxes_np = np.array(boxes).tolist()\n",
    "    confidences_np = np.array(confidences).tolist()\n",
    "\n",
    "    result = cv2.dnn.NMSBoxes(boxes_np, confidences_np, 0.3, 0.2)\n",
    "\n",
    "    if len(result) > 0:\n",
    "        flattened_result = result.flatten()\n",
    "        for ind in flattened_result:\n",
    "            x, y, w, h = boxes_np[ind]\n",
    "            bb_conf = int(confidences_np[ind] * 100)\n",
    "            classes_id = classes[ind]\n",
    "            class_name = labels1[classes_id]\n",
    "            text = f'{class_name}:{bb_conf}%'\n",
    "            if(('officer' in class_name.lower() or 'text' in class_name.lower()) and bb_conf>65):\n",
    "                cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.putText(image, text, (x, y+20), cv2.FONT_HERSHEY_PLAIN, 0.7, (255, 255, 255), 1)\n",
    "                print('police = ',text)\n",
    "                pol=1\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    INPUT_WH_YOLO = 640\n",
    "    blob = cv2.dnn.blobFromImage(input_image, 1/255, (INPUT_WH_YOLO, INPUT_WH_YOLO), swapRB=True, crop=False)\n",
    "    yolo.setInput(blob)\n",
    "    preds = yolo.forward()\n",
    "\n",
    "    detections = preds[0]\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classes = []\n",
    "\n",
    "    image_w, image_h = input_image.shape[:2]\n",
    "    x_fac = image_w / INPUT_WH_YOLO\n",
    "    y_fac = image_h / INPUT_WH_YOLO\n",
    "\n",
    "    for i in range(len(detections)):\n",
    "        row = detections[i]\n",
    "        confidence = row[4]\n",
    "        if confidence > 0:\n",
    "            class_score = row[5:].max()\n",
    "            class_id = row[5:].argmax()\n",
    "            if class_score > 0.25:\n",
    "                cx, cy, w, h = row[0:4]\n",
    "                left = int((cx - 0.5 * w) * x_fac)\n",
    "                top = int((cy - 0.5 * h) * y_fac)\n",
    "                width = int(w * x_fac)\n",
    "                height = int(h * y_fac)\n",
    "                box = np.array([left, top, width, height])\n",
    "                confidences.append(confidence)\n",
    "                boxes.append(box)\n",
    "                classes.append(class_id)\n",
    "\n",
    "    boxes_np = np.array(boxes).tolist()\n",
    "    confidences_np = np.array(confidences).tolist()\n",
    "\n",
    "    result = cv2.dnn.NMSBoxes(boxes_np, confidences_np, 0.3, 0.2)\n",
    "    print(result)\n",
    "    if len(result) > 0:\n",
    "        flattened_result = result.flatten()\n",
    "        for ind in flattened_result:\n",
    "            x, y, w, h = boxes_np[ind]\n",
    "            bb_conf = int(confidences_np[ind] * 100)\n",
    "            classes_id = classes[ind]\n",
    "            class_name = labels[classes_id]\n",
    "            text=class_name+' = '+str(bb_conf)+'%'\n",
    "            # Draw a rectangle around the gun and label it as 'gun'=='knife'\n",
    "            #if('gun' in class_name):\n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 255), 2)\n",
    "            cv2.putText(image, text, (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 255, 255), 1)\n",
    "                #cv2.putText(image, f'Gun {bb_conf}% confidence', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 255, 255), 1)\n",
    "    '''else:\n",
    "        yolo_knife.setInput(blob)\n",
    "        preds = yolo_knife.forward()\n",
    "\n",
    "        detections = preds[0]\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        classes = []\n",
    "\n",
    "        image_w, image_h = input_image.shape[:2]\n",
    "        x_fac = image_w / INPUT_WH_YOLO\n",
    "        y_fac = image_h / INPUT_WH_YOLO\n",
    "\n",
    "        for i in range(len(detections)):\n",
    "            row = detections[i]\n",
    "            confidence = row[4]\n",
    "            if confidence > 0:\n",
    "                class_score = row[5:].max()\n",
    "                class_id = row[5:].argmax()\n",
    "                if class_score > 0.25:\n",
    "                    cx, cy, w, h = row[0:4]\n",
    "                    left = int((cx - 0.5 * w) * x_fac)\n",
    "                    top = int((cy - 0.5 * h) * y_fac)\n",
    "                    width = int(w * x_fac)\n",
    "                    height = int(h * y_fac)\n",
    "                    box = np.array([left, top, width, height])\n",
    "                    confidences.append(confidence)\n",
    "                    boxes.append(box)\n",
    "                    classes.append(class_id)\n",
    "\n",
    "        boxes_np = np.array(boxes).tolist()\n",
    "        confidences_np = np.array(confidences).tolist()\n",
    "\n",
    "        result = cv2.dnn.NMSBoxes(boxes_np, confidences_np, 0.3, 0.2)\n",
    "        print(result)\n",
    "        if len(result) > 0:\n",
    "            flattened_result = result.flatten()\n",
    "            for ind in flattened_result:\n",
    "                x, y, w, h = boxes_np[ind]\n",
    "                bb_conf = int(confidences_np[ind] * 100)\n",
    "                classes_id = classes[ind]\n",
    "                class_name = labels[classes_id]\n",
    "\n",
    "                # Draw a rectangle around the gun and label it as 'gun'\n",
    "                #print('bb_conf = ',bb_conf)\n",
    "                cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "                cv2.putText(image, 'Knife', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 255), 1)\n",
    "                #cv2.putText(image, f'Knife {bb_conf}% confidence', (x, y+10), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 255), 1)'''      \n",
    "\n",
    "    # Detect emotion of the person in the frame\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_roi = gray_frame[y:y + h, x:x + w]\n",
    "        resized_face = cv2.resize(face_roi, (48, 48), interpolation=cv2.INTER_AREA)\n",
    "        normalized_face = resized_face / 255.0\n",
    "        reshaped_face = normalized_face.reshape(1, 48, 48, 1)\n",
    "        preds = model.predict(reshaped_face)[0]\n",
    "        emotion_idx = preds.argmax()\n",
    "        emotion = emotion_labels[emotion_idx]\n",
    "        print(emotion)\n",
    "        confidence_percentage = preds[emotion_idx] * 100\n",
    "        # If gun is detected and emotion is sad or angry, stop running the camera after 3 seconds\n",
    "        if start_time is None:\n",
    "            start_time = time.time()  # Store the current time\n",
    "        else:\n",
    "            current_time = time.time()  # Get the current time\n",
    "            elapsed_time = current_time - start_time  # Calculate the elapsed time\n",
    "            if elapsed_time > 60:  # Check if 30 seconds have passed\n",
    "                print(emotion)\n",
    "                cap.release()  # Release the camera\n",
    "                cv2.destroyAllWindows()  # Close the windows\n",
    "                break  # Break out of the while loop\n",
    "\n",
    "        # Draw a rectangle around the emotion and label it with predicted emotion\n",
    "        color = (0,0,255) if emotion == 'angry' else (0, 0, 255)\n",
    "        text = emotion\n",
    "        #text = f'{emotion}: {confidence_percentage:.2f}%'\n",
    "        if(confidence_percentage<90):\n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), color, 2)\n",
    "            cv2.putText(image, text, (x, y+100), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "    cv2.imshow(\"Video\", image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "from datetime import datetime\n",
    "current_time = datetime.now()\n",
    "hours = current_time.hour\n",
    "minutes = current_time.minute\n",
    "seconds = current_time.second\n",
    "day=month=current_time.day\n",
    "month=current_time.month\n",
    "year=current_time.year\n",
    "\n",
    "# Print the extracted values\n",
    "print(f\"Current Time: {current_time}\")\n",
    "print(f\"Hours: {hours}\")\n",
    "print(f\"Minutes: {minutes}\")\n",
    "print(f\"Seconds: {seconds}\")\n",
    "print(f\"Day: {day}\")\n",
    "print(f\"Month: {month}\")\n",
    "print(f\"Year: {year}\")\n",
    "\n",
    "result = get_lat_long_by_ip()\n",
    "latitude, longitude = result\n",
    "print('latitude = ',latitude)\n",
    "print('longitude = ',longitude)\n",
    "x, y = convert_lat_lon_to_xy(latitude, longitude)\n",
    "print(f\"X Coordinate: {x}, Y Coordinate: {y}\")\n",
    "\n",
    "# Replace these values with your own data\n",
    "#new_data = np.array([[0,0,latitude, longitude, x, y, year, month, day, hours, minutes,seconds]])\n",
    "new_data = np.array([[1,2,41.830482, -87.621752, 1182280, 1881621, 2023, 12, 15, 14, 30,0]])\n",
    "#new_data=np.array([[0,1,41.747610,-87.549179,1198234.0,1851595.0,2020,7,1,10,16,0]])\n",
    "# Standardize the new data using the same scaler\n",
    "#new_data_standardized = scaler.transform(new_data)\n",
    "\n",
    "# Make predictions for the new data\n",
    "new_data_prediction = loaded_model.predict(new_data)\n",
    "\n",
    "print(f'Predicted Alert for the new data: {new_data_prediction[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping between Original and Encoded Values:\n",
    "   Original Encoded\n",
    "0     angry       0\n",
    "1   disgust       1\n",
    "2      fear       2\n",
    "3     happy       3\n",
    "4   neutral       4\n",
    "5       sad       5\n",
    "6  surprise       6\n",
    "\n",
    "0     gun       0\n",
    "1   knife       1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude: 41.0878, Longitude: -87.2785\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_lat_long_by_ip():\n",
    "    try:\n",
    "        response = requests.get('https://ipinfo.io/json')\n",
    "        data = response.json()\n",
    "\n",
    "        if 'loc' in data:\n",
    "            latitude, longitude = map(float, data['loc'].split(','))\n",
    "            latitude+=28\n",
    "            longitude+=7\n",
    "            longitude*=-1\n",
    "            return latitude, longitude\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "result = get_lat_long_by_ip()\n",
    "\n",
    "if result:\n",
    "    latitude, longitude = result\n",
    "    print(f\"Latitude: {latitude}, Longitude: {longitude}\")\n",
    "else:\n",
    "    print(\"Could not retrieve location information.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140059.4051471204 1854810.5772076007\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def convert_lat_lon_to_xy(latitude, longitude):\n",
    "    # Radius of the Earth in meters\n",
    "    R = 6371000.0\n",
    "    \n",
    "    # Convert latitude and longitude to radians\n",
    "    lat_rad = math.radians(latitude)\n",
    "    lon_rad = math.radians(longitude)\n",
    "    \n",
    "    #y Calculate x and y coordinates using Mercator projection\n",
    "    x = R * lon_rad\n",
    "    y = R * math.log(math.tan(math.pi / 4 + lat_rad / 2))\n",
    "    \n",
    "    return x, y\n",
    "x,y=convert_lat_lon_to_xy(41.0878,-87.2785)\n",
    "x*=-1\n",
    "x-=8564867\n",
    "y-=3164867\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>Block</th>\n",
       "      <th>IUCR</th>\n",
       "      <th>Primary Type</th>\n",
       "      <th>Description</th>\n",
       "      <th>Location Description</th>\n",
       "      <th>Arrest</th>\n",
       "      <th>Domestic</th>\n",
       "      <th>...</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Location</th>\n",
       "      <th>Alert</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Second</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12045583</td>\n",
       "      <td>JD226426</td>\n",
       "      <td>2020-05-07 10:24:00</td>\n",
       "      <td>12982</td>\n",
       "      <td>292</td>\n",
       "      <td>THEFT</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>41.830482</td>\n",
       "      <td>-87.621752</td>\n",
       "      <td>(41.830481843, -87.621751752)</td>\n",
       "      <td>Danger</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12031001</td>\n",
       "      <td>JD209965</td>\n",
       "      <td>2020-04-16 05:00:00</td>\n",
       "      <td>2355</td>\n",
       "      <td>222</td>\n",
       "      <td>BATTERY</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>41.836310</td>\n",
       "      <td>-87.639624</td>\n",
       "      <td>(41.836310224, -87.639624112)</td>\n",
       "      <td>Safe</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12093529</td>\n",
       "      <td>JD282112</td>\n",
       "      <td>2020-07-01 10:16:00</td>\n",
       "      <td>23418</td>\n",
       "      <td>6</td>\n",
       "      <td>ASSAULT</td>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>41.747610</td>\n",
       "      <td>-87.549179</td>\n",
       "      <td>(41.747609555, -87.549179329)</td>\n",
       "      <td>Safe</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12178140</td>\n",
       "      <td>JD381597</td>\n",
       "      <td>2020-09-27 23:29:00</td>\n",
       "      <td>21138</td>\n",
       "      <td>222</td>\n",
       "      <td>BATTERY</td>\n",
       "      <td>2</td>\n",
       "      <td>125</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>41.774878</td>\n",
       "      <td>-87.671375</td>\n",
       "      <td>(41.77487752, -87.671374872)</td>\n",
       "      <td>Danger</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12126129</td>\n",
       "      <td>JD321064</td>\n",
       "      <td>2020-08-04 20:28:00</td>\n",
       "      <td>23470</td>\n",
       "      <td>59</td>\n",
       "      <td>WEAPONS VIOLATION</td>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>41.746221</td>\n",
       "      <td>-87.658477</td>\n",
       "      <td>(41.746220584, -87.658477307)</td>\n",
       "      <td>Safe</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID Case Number                 Date  Block  IUCR       Primary Type  \\\n",
       "0  12045583    JD226426  2020-05-07 10:24:00  12982   292              THEFT   \n",
       "1  12031001    JD209965  2020-04-16 05:00:00   2355   222            BATTERY   \n",
       "2  12093529    JD282112  2020-07-01 10:16:00  23418     6            ASSAULT   \n",
       "3  12178140    JD381597  2020-09-27 23:29:00  21138   222            BATTERY   \n",
       "4  12126129    JD321064  2020-08-04 20:28:00  23470    59  WEAPONS VIOLATION   \n",
       "\n",
       "   Description  Location Description  Arrest  Domestic  ...   Latitude  \\\n",
       "0            2                    17   False     False  ...  41.830482   \n",
       "1            2                    17    True     False  ...  41.836310   \n",
       "2            0                   147    True     False  ...  41.747610   \n",
       "3            2                   125   False     False  ...  41.774878   \n",
       "4            0                   147    True     False  ...  41.746221   \n",
       "\n",
       "   Longitude                       Location   Alert Month  Day  Hour  Minute  \\\n",
       "0 -87.621752  (41.830481843, -87.621751752)  Danger     5    7    10      24   \n",
       "1 -87.639624  (41.836310224, -87.639624112)    Safe     4   16     5       0   \n",
       "2 -87.549179  (41.747609555, -87.549179329)    Safe     7    1    10      16   \n",
       "3 -87.671375   (41.77487752, -87.671374872)  Danger     9   27    23      29   \n",
       "4 -87.658477  (41.746220584, -87.658477307)    Safe     8    4    20      28   \n",
       "\n",
       "  Second  emotion  \n",
       "0      0        6  \n",
       "1      0        3  \n",
       "2      0        0  \n",
       "3      0        0  \n",
       "4      0        3  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "df=pd.read_csv('Updated_dataset_Crime1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8444605533405823\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Danger       0.87      0.95      0.91     34715\n",
      "        Safe       0.53      0.28      0.36      6670\n",
      "\n",
      "    accuracy                           0.84     41385\n",
      "   macro avg       0.70      0.61      0.64     41385\n",
      "weighted avg       0.82      0.84      0.82     41385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "df=pd.read_csv('Updated_dataset_Crime1.csv')\n",
    "# Assuming df is your DataFrame containing the dataset\n",
    "\n",
    "# Splitting the data into features (X) and target variable (y)\n",
    "X = df[['Description','emotion','Latitude','Longitude','X Coordinate','Y Coordinate','Year','Month','Day','Hour','Minute','Second']]\n",
    "y = df['Alert']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
    "# Creating and training the RandomForestClassifier model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "# Printing the classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print the confusion matrix if needed\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "#print(confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "#joblib.dump(model, 'RF_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Safe'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = joblib.load(\"C:/Users/Gnana Bharathi S/Downloads/Dec 14/RF1_81.pkl\")\n",
    "new_data=np.array([[0,3,41.840482,-87.621752,1178180.0,1881621.0,2020,10,20,12,23,0]])\n",
    "# Standardize the new data using the same scaler\n",
    "#new_data_standardized = scaler.transform(new_data)\n",
    "\n",
    "# Make predictions for the new data\n",
    "new_data_prediction = loaded_model.predict(new_data)\n",
    "new_data_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9644073939833273\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Danger       0.97      0.99      0.98     34715\n",
      "        Safe       0.94      0.84      0.88      6670\n",
      "\n",
      "    accuracy                           0.96     41385\n",
      "   macro avg       0.95      0.91      0.93     41385\n",
      "weighted avg       0.96      0.96      0.96     41385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "df=pd.read_csv('Updated_dataset_Crime1.csv')\n",
    "# Assuming df is your DataFrame containing the dataset\n",
    "\n",
    "# Splitting the data into features (X) and target variable (y)\n",
    "X = df[['Description','emotion','Latitude','Longitude','X Coordinate','Y Coordinate','Year','Month','Day','Hour','Minute','Second']]\n",
    "y = df['Alert']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "# Printing the classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print the confusion matrix if needed\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "#print(confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9644073939833273\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Danger       0.97      0.99      0.98     34715\n",
      "        Safe       0.94      0.84      0.88      6670\n",
      "\n",
      "    accuracy                           0.96     41385\n",
      "   macro avg       0.95      0.91      0.93     41385\n",
      "weighted avg       0.96      0.96      0.96     41385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "df=pd.read_csv('Updated_dataset_Crime1.csv')\n",
    "# Assuming df is your DataFrame containing the dataset\n",
    "\n",
    "# Splitting the data into features (X) and target variable (y)\n",
    "X = df[['Description','emotion','Latitude','Longitude','X Coordinate','Y Coordinate','Year','Month','Day','Hour','Minute','Second']]\n",
    "y = df['Alert']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "# Printing the classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print the confusion matrix if needed\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "#print(confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Alert for the new data: Safe\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#new_data = np.array([[0,0,41.747610,-87.549179,1198234.0,1851595.0,2020,7,1,10,16,0]])\n",
    "new_data=np.array([[0,3,41.840482,-87.621752,1178180.0,1881621.0,2020,10,20,12,23,0]])\n",
    "# Standardize the new data using the same scaler\n",
    "#new_data_standardized = scaler.transform(new_data)\n",
    "\n",
    "# Make predictions for the new data\n",
    "new_data_prediction = loaded_model.predict(new_data)\n",
    "print(f'Predicted Alert for the new data: {new_data_prediction[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Alert for the new data: Safe\n"
     ]
    }
   ],
   "source": [
    "new_data_prediction = loaded_model.predict(new_data)\n",
    "print(f'Predicted Alert for the new data: {new_data_prediction[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time: 2024-02-24 20:42:36.647970\n",
      "Hours: 20\n",
      "Minutes: 42\n",
      "Seconds: 36\n",
      "Day: 24\n",
      "Month: 2\n",
      "Year: 2024\n",
      "latitude =  13.0878\n",
      "longitude =  80.2785\n",
      "X Coordinate: 8926561.918635208, Y Coordinate: 1468120.358491257\n",
      "Predicted Alert for the new data: Safe\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "current_time = datetime.now()\n",
    "hours = current_time.hour\n",
    "minutes = current_time.minute\n",
    "seconds = current_time.second\n",
    "day=month=current_time.day\n",
    "month=current_time.month\n",
    "year=current_time.year\n",
    "\n",
    "# Print the extracted values\n",
    "print(f\"Current Time: {current_time}\")\n",
    "print(f\"Hours: {hours}\")\n",
    "print(f\"Minutes: {minutes}\")\n",
    "print(f\"Seconds: {seconds}\")\n",
    "print(f\"Day: {day}\")\n",
    "print(f\"Month: {month}\")\n",
    "print(f\"Year: {year}\")\n",
    "\n",
    "result = get_lat_long_by_ip()\n",
    "latitude, longitude = result\n",
    "print('latitude = ',latitude)\n",
    "print('longitude = ',longitude)\n",
    "x, y = convert_lat_lon_to_xy(latitude, longitude)\n",
    "print(f\"X Coordinate: {x}, Y Coordinate: {y}\")\n",
    "\n",
    "# Replace these values with your own data\n",
    "#new_data = np.array([[0,0,latitude, longitude, x, y, year, month, day, hours, minutes,seconds]])\n",
    "#new_data = np.array([[1,2,41.830482, -87.621752, 1182280, 1881621, 2023, 12, 15, 14, 30,0]])\n",
    "new_data=np.array([[0,1,41.747610,-87.549179,1198234.0,1851595.0,2020,7,1,10,16,0]])\n",
    "# Standardize the new data using the same scaler\n",
    "#new_data_standardized = scaler.transform(new_data)\n",
    "\n",
    "# Make predictions for the new data\n",
    "new_data_prediction = loaded_model.predict(new_data)\n",
    "\n",
    "print(f'Predicted Alert for the new data: {new_data_prediction[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
